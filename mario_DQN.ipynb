{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mario_DQN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satann3456/DQN_mario/blob/master/mario_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ZDflbUl5q-AP",
        "colab_type": "code",
        "outputId": "adc10a42-1a6d-4662-f55d-5f2b4d51bf97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1315
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get -qq -y install libcusparse8.0 libnvrtc8.0 libnvtoolsext1 > /dev/null\n",
        "!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n",
        "\n",
        "!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\n",
        "!pip install pyglet\n",
        "!pip install pyopengl\n",
        "!pip install gym[classic_control]\n",
        "!pip install keras-rl\n",
        "!pip install gym-super-mario-bros"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Unable to locate package libcusparse8.0\n",
            "E: Couldn't find any package by glob 'libcusparse8.0'\n",
            "E: Couldn't find any package by regex 'libcusparse8.0'\n",
            "E: Unable to locate package libnvrtc8.0\n",
            "E: Couldn't find any package by glob 'libnvrtc8.0'\n",
            "E: Couldn't find any package by regex 'libnvrtc8.0'\n",
            "Requirement already satisfied: pyglet in /usr/local/lib/python3.6/dist-packages (1.3.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet) (0.16.0)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.6/dist-packages (3.1.0)\n",
            "Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.6/dist-packages (0.10.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[classic_control]) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[classic_control]) (1.14.6)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym[classic_control]) (2.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[classic_control]) (1.11.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[classic_control]) (1.3.2)\n",
            "Requirement already satisfied: PyOpenGL in /usr/local/lib/python3.6/dist-packages (from gym[classic_control]) (3.1.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[classic_control]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[classic_control]) (2018.11.29)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[classic_control]) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[classic_control]) (1.22)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym[classic_control]) (0.16.0)\n",
            "Collecting keras-rl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/87/4b57eff8e4bd834cea0a75cd6c58198c9e42be29b600db9c14fafa72ec07/keras-rl-0.4.2.tar.gz (40kB)\n",
            "\u001b[K    100% |████████████████████████████████| 40kB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.6/dist-packages (from keras-rl) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.14.6)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.11.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.0.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.0.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (2.8.0)\n",
            "Building wheels for collected packages: keras-rl\n",
            "  Running setup.py bdist_wheel for keras-rl ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/7d/4d/84/9254c9f2e8f51865cb0dac8e79da85330c735551d31f73c894\n",
            "Successfully built keras-rl\n",
            "Installing collected packages: keras-rl\n",
            "Successfully installed keras-rl-0.4.2\n",
            "Collecting gym-super-mario-bros\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/56/906e2f956f3aad3d873b4a3a3591806b837057b29aa100e1af9afafea462/gym_super_mario_bros-7.0.1-py2.py3-none-any.whl (195kB)\n",
            "\u001b[K    100% |████████████████████████████████| 204kB 27.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python>=3.4.0.12 in /usr/local/lib/python3.6/dist-packages (from gym-super-mario-bros) (3.4.5.20)\n",
            "Collecting pygame>=1.9.3 (from gym-super-mario-bros)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/5e/fb7c85304ad1fd52008fd25fce97a7f59e6147ae97378afc86cf0f5d9146/pygame-1.9.4-cp36-cp36m-manylinux1_x86_64.whl (12.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 12.1MB 2.3MB/s \n",
            "\u001b[?25hCollecting nes-py>=5.0.0 (from gym-super-mario-bros)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/40/3ab86e1f473a7f4a88befc863d1144683c49d94f42dbd1167733ef07e89b/nes_py-5.0.2.tar.gz (73kB)\n",
            "\u001b[K    100% |████████████████████████████████| 81kB 28.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.2 in /usr/local/lib/python3.6/dist-packages (from gym-super-mario-bros) (1.14.6)\n",
            "Requirement already satisfied: tqdm>=4.19.5 in /usr/local/lib/python3.6/dist-packages (from gym-super-mario-bros) (4.28.1)\n",
            "Requirement already satisfied: matplotlib>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from gym-super-mario-bros) (2.1.2)\n",
            "Requirement already satisfied: pyglet>=1.3.2 in /usr/local/lib/python3.6/dist-packages (from gym-super-mario-bros) (1.3.2)\n",
            "Requirement already satisfied: gym>=0.10.9 in /usr/local/lib/python3.6/dist-packages (from nes-py>=5.0.0->gym-super-mario-bros) (0.10.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.2->gym-super-mario-bros) (0.10.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.2->gym-super-mario-bros) (1.11.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.2->gym-super-mario-bros) (2018.9)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.2->gym-super-mario-bros) (2.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.2->gym-super-mario-bros) (2.5.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.3.2->gym-super-mario-bros) (0.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym>=0.10.9->nes-py>=5.0.0->gym-super-mario-bros) (1.1.0)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.10.9->nes-py>=5.0.0->gym-super-mario-bros) (2.18.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym>=0.10.9->nes-py>=5.0.0->gym-super-mario-bros) (2018.11.29)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym>=0.10.9->nes-py>=5.0.0->gym-super-mario-bros) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym>=0.10.9->nes-py>=5.0.0->gym-super-mario-bros) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym>=0.10.9->nes-py>=5.0.0->gym-super-mario-bros) (1.22)\n",
            "Building wheels for collected packages: nes-py\n",
            "  Running setup.py bdist_wheel for nes-py ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3c/a3/f8/56f889fbb151951c518797e32f630cd944d197a3e16fb16415\n",
            "Successfully built nes-py\n",
            "Installing collected packages: pygame, nes-py, gym-super-mario-bros\n",
            "Successfully installed gym-super-mario-bros-7.0.1 nes-py-5.0.2 pygame-1.9.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yigWGvK2zYCB",
        "colab_type": "code",
        "outputId": "121d3b1d-da3f-437f-9b40-681f8f43ce37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools > /dev/null\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse > /dev/null\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Generate creds for the Drive FUSE library.\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K4FOlZyJ0aMD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 報酬獲得方法を変更する場合"
      ]
    },
    {
      "metadata": {
        "id": "HfL_75vf0uV9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "/usr/local/lib/python3.6/dist-packages/gym_super_mario_brosにあるsmb_env.pyをダウンロードして中の報酬値の設定を変更する\n",
        "\n",
        "（報酬値の変更の仕方はsmb_env.pyのコード読めばわかるはず）\n",
        "\n",
        "そして上書き更新する\n",
        "\n",
        "colaboratoryが再起動するたびに実行しないと報酬値が混ざるので注意"
      ]
    },
    {
      "metadata": {
        "id": "vw-Nq_4WukYj",
        "colab_type": "code",
        "outputId": "215c2a1b-1d12-49fd-ad6c-7d7089cece10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "cell_type": "code",
      "source": [
        "!ls /usr/local/lib/python3.6/dist-packages/gym_super_mario_bros/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "actions.py  __init__.py  _registration.py  smb_env.py\n",
            "_app\t    __pycache__  _roms\t\t   tests\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nUtOYPIOHsa-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp smb_env.py /usr/local/lib/python3.6/dist-packages/gym_super_mario_bros"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kntF3jO00gZh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### トレーニング"
      ]
    },
    {
      "metadata": {
        "id": "TduWEH69j__c",
        "colab_type": "code",
        "outputId": "d5ffaa5d-304c-438f-c18d-88874a0e75e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import keras\n",
        "\n",
        "from PIL import Image\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Conv2D, Reshape, MaxPooling2D, Permute\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import plot_model\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy, MaxBoltzmannQPolicy, LinearAnnealedPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.core import Processor\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
        "\n",
        "from nes_py.wrappers import BinarySpaceToDiscreteSpaceEnv\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
        "\n",
        "from gym import wrappers\n",
        "\n",
        "#動画や報酬値の結果を保存したいフォルダを選択。毎回変えないと前のデータが消えるので注意\n",
        "folder = './drive/ProX/mario7_log21'\n",
        "\n",
        "#ここを変えればgym_super_mario_brosのEnvironmentsを変更できる。種類はgym_super_mario_brosのgithub参照\n",
        "ENV_NAME = 'SuperMarioBros-v0' \n",
        "\n",
        "INPUT_SHAPE = (84, 84) #入力画像のサイズを設定\n",
        "WINDOW_LENGTH = 4 \n",
        "\n",
        "# 入力画像をグレースケールにしたり、INPUT SHAPEの値にサイズを変更\n",
        "# また、報酬値を1 ∼ -1の値に正規化する\n",
        "class GameProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        assert observation.ndim == 3\n",
        "        img = Image.fromarray(observation)\n",
        "        img = img.resize(INPUT_SHAPE).convert('L')\n",
        "        processed_observation = np.array(img)\n",
        "        assert processed_observation.shape == INPUT_SHAPE\n",
        "        return processed_observation.astype('uint8')\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        return np.clip(reward, -1., 1.)\n",
        "\n",
        "env = gym_super_mario_bros.make(ENV_NAME)\n",
        "env = BinarySpaceToDiscreteSpaceEnv(env, SIMPLE_MOVEMENT)\n",
        "\n",
        "# ビデオを保存する\n",
        "# intervalの値を変更すれば動画を保存するエピソードの間隔を設定できる\n",
        "interval = 10\n",
        "env = wrappers.Monitor(env, folder, force=True, video_callable=(lambda ep: ep % interval == 0))\n",
        "\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = len(SIMPLE_MOVEMENT)\n",
        "\n",
        "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
        "model = Sequential()\n",
        "model.add(Permute((2,3,1),input_shape=input_shape))\n",
        "model.add(Conv2D(32, (8, 8), strides=(4, 4)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (4, 4), strides=(2, 2)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3), strides=(1, 1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))\n",
        "print(model.summary())\n",
        "\n",
        "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
        "processor = GameProcessor()\n",
        "\n",
        "# εグリーディ法を使用する。epsが高いとランダム動作をとる確率が高くなるので徐々に低い値にする(0∼1の範囲)\n",
        "# 他の手法もあるのでkeras-rlのgithubのページ参照\n",
        "policy = EpsGreedyQPolicy(eps=.9)\n",
        "#policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1.,value_test=.05, value_min=.1, nb_steps=400000)\n",
        "\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=500, \n",
        "             processor=processor, target_model_update=10000, policy=policy)\n",
        "dqn.compile(Adam(lr=0.00025), metrics=['mae'])\n",
        "\n",
        "#dqn.load_weights('./drive/ProX/dqn_eps2_{}_weights.h5f'.format(ENV_NAME))\n",
        "checkpoint_weights_filename = folder + '/dqn_' + ENV_NAME + '_weights_{step}.h5f'\n",
        "log_filename = folder + '/dqn_{}_log.json'.format(ENV_NAME)\n",
        "\n",
        "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000)]\n",
        "callbacks += [FileLogger(log_filename, interval=100)]\n",
        "dqn.fit(env,callbacks=callbacks, nb_steps=500000, visualize=False, verbose=2)\n",
        "#dqn.save_weights('./drive/ProX/dqn_eps2_{}_weights.h5f'.format(ENV_NAME), overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: Could not seed environment <SuperMarioBrosEnv<SuperMarioBros-v0>>\u001b[0m\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "permute_1 (Permute)          (None, 84, 84, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 20, 20, 32)        8224      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 20, 20, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 9, 9, 64)          32832     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 9, 9, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 7, 7, 64)          36928     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               1606144   \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 7)                 3591      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 7)                 0         \n",
            "=================================================================\n",
            "Total params: 1,687,719\n",
            "Trainable params: 1,687,719\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Training for 500000 steps ...\n",
            "   2358/500000: episode: 1, duration: 68.315s, episode steps: 2358, steps per second: 35, episode reward: 627.000, mean reward: 0.266 [-1.000, 1.000], mean action: 2.462 [0.000, 6.000], mean observation: 138.138 [0.000, 252.000], loss: 0.040527, mean_absolute_error: 0.270722, mean_q: 0.374097\n",
            "   3855/500000: episode: 2, duration: 43.379s, episode steps: 1497, steps per second: 35, episode reward: 839.000, mean reward: 0.560 [-1.000, 1.000], mean action: 2.807 [0.000, 6.000], mean observation: 137.863 [0.000, 252.000], loss: 0.043448, mean_absolute_error: 0.366286, mean_q: 0.483723\n",
            "   6894/500000: episode: 3, duration: 79.772s, episode steps: 3039, steps per second: 38, episode reward: 612.000, mean reward: 0.201 [-1.000, 1.000], mean action: 2.642 [0.000, 6.000], mean observation: 137.436 [0.000, 252.000], loss: 0.036204, mean_absolute_error: 0.360531, mean_q: 0.444696\n",
            "   7304/500000: episode: 4, duration: 10.901s, episode steps: 410, steps per second: 38, episode reward: 376.000, mean reward: 0.917 [-1.000, 1.000], mean action: 2.973 [0.000, 6.000], mean observation: 137.992 [0.000, 252.000], loss: 0.033557, mean_absolute_error: 0.354379, mean_q: 0.437026\n",
            "   8657/500000: episode: 5, duration: 35.833s, episode steps: 1353, steps per second: 38, episode reward: 850.000, mean reward: 0.628 [-1.000, 1.000], mean action: 3.148 [0.000, 6.000], mean observation: 138.549 [0.000, 252.000], loss: 0.035893, mean_absolute_error: 0.381676, mean_q: 0.466366\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RGUt5lnkqS6h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### モデルの画像化"
      ]
    },
    {
      "metadata": {
        "id": "bLaUkLUMqb1l",
        "colab_type": "code",
        "outputId": "46f0a38c-3a6e-452c-ad7f-af290c026783",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install graphviz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (0.10.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pXVglSANqlHE",
        "colab_type": "code",
        "outputId": "b8d2f52a-354b-4e6f-d1c5-c67e8c83c367",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install pydot_ng"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydot_ng in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from pydot_ng) (2.3.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "acebe648-f9aa-4f6a-f0ea-0cc4a32d1e95",
        "id": "CcUioKCAqZah",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import keras\n",
        "\n",
        "from PIL import Image\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Conv2D, Reshape, MaxPooling2D, Permute\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import plot_model\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy, MaxBoltzmannQPolicy, LinearAnnealedPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.core import Processor\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
        "\n",
        "from nes_py.wrappers import BinarySpaceToDiscreteSpaceEnv\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
        "\n",
        "from gym import wrappers\n",
        "\n",
        "#動画や報酬値の結果を保存したいフォルダを選択。毎回変えないと前のデータが消えるので注意\n",
        "folder = './drive/ProX/mario7_log21'\n",
        "\n",
        "#ここを変えればgym_super_mario_brosのEnvironmentsを変更できる。種類はgym_super_mario_brosのgithub参照\n",
        "ENV_NAME = 'SuperMarioBros-v0' \n",
        "\n",
        "INPUT_SHAPE = (84, 84) #入力画像のサイズを設定\n",
        "WINDOW_LENGTH = 4 \n",
        "\n",
        "# 入力画像をグレースケールにしたり、INPUT SHAPEの値にサイズを変更\n",
        "# また、報酬値を1 ∼ -1の値に正規化する\n",
        "class GameProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        assert observation.ndim == 3\n",
        "        img = Image.fromarray(observation)\n",
        "        img = img.resize(INPUT_SHAPE).convert('L')\n",
        "        processed_observation = np.array(img)\n",
        "        assert processed_observation.shape == INPUT_SHAPE\n",
        "        return processed_observation.astype('uint8')\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        return np.clip(reward, -1., 1.)\n",
        "\n",
        "env = gym_super_mario_bros.make(ENV_NAME)\n",
        "env = BinarySpaceToDiscreteSpaceEnv(env, SIMPLE_MOVEMENT)\n",
        "\n",
        "# ビデオを保存する\n",
        "# intervalの値を変更すれば動画を保存するエピソードの間隔を設定できる\n",
        "interval = 10\n",
        "env = wrappers.Monitor(env, folder, force=True, video_callable=(lambda ep: ep % interval == 0))\n",
        "\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = len(SIMPLE_MOVEMENT)\n",
        "\n",
        "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
        "model = Sequential()\n",
        "model.add(Permute((2,3,1),input_shape=input_shape))\n",
        "model.add(Conv2D(32, (8, 8), strides=(4, 4)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (4, 4), strides=(2, 2)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3), strides=(1, 1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))\n",
        "print(model.summary())\n",
        "\n",
        "plot_model(model, to_file='model.png')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-3986282ffaeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdqn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBoltzmannQPolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEpsGreedyQPolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxBoltzmannQPolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinearAnnealedPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequentialMemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rl'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "zzSXlyzPkIul",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 試行錯誤したコード"
      ]
    },
    {
      "metadata": {
        "id": "5DNXDeYUYlIK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### テスト"
      ]
    },
    {
      "metadata": {
        "id": "uUZu64ivX9wr",
        "colab_type": "code",
        "outputId": "a71948d0-3838-4e48-cd38-3275e448fd7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import keras\n",
        "#import pydot\n",
        "\n",
        "from PIL import Image\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Conv2D, Reshape, MaxPooling2D, Permute\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import plot_model\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy, MaxBoltzmannQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.core import Processor\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
        "\n",
        "from nes_py.wrappers import BinarySpaceToDiscreteSpaceEnv\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
        "\n",
        "from gym import wrappers\n",
        "\n",
        "from keras import backend as K # 柴田\n",
        "\n",
        "\n",
        "ENV_NAME = 'SuperMarioBros-v0'\n",
        "\n",
        "INPUT_SHAPE = (84, 84)\n",
        "WINDOW_LENGTH = 4\n",
        "\n",
        "class GameProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        assert observation.ndim == 3  # (height, width, channel)\n",
        "        img = Image.fromarray(observation)\n",
        "        img = img.resize(INPUT_SHAPE).convert('L')  # resize and convert to grayscale\n",
        "        processed_observation = np.array(img)\n",
        "        assert processed_observation.shape == INPUT_SHAPE\n",
        "        return processed_observation.astype('uint8')  # saves storage in experience memory\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        # We could perform this processing step in `process_observation`. In this case, however,\n",
        "        # we would need to store a `float32` array instead, which is 4x more memory intensive than\n",
        "        # an `uint8` array. This matters if we store 1M observations.\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        return np.clip(reward, -1., 1.)\n",
        "\n",
        "env = gym_super_mario_bros.make(ENV_NAME)\n",
        "env = BinarySpaceToDiscreteSpaceEnv(env, SIMPLE_MOVEMENT)\n",
        "\n",
        "env = wrappers.Monitor(env, './drive/ProX/tst/mario_log15', force=True, video_callable=(lambda ep: ep % 1 == 0))\n",
        "\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = len(SIMPLE_MOVEMENT)\n",
        "\n",
        "\n",
        "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
        "model = Sequential()\n",
        "model.add( Permute((2,3,1), input_shape=input_shape ) )\n",
        "model.add(Conv2D(32, (8, 8), strides=(4, 4)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (4, 4), strides=(2, 2)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3), strides=(1, 1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "memory = SequentialMemory(limit=30000, window_length=WINDOW_LENGTH)\n",
        "processor = GameProcessor()\n",
        "policy = EpsGreedyQPolicy(eps=.05)\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=500, \n",
        "             processor=processor, target_model_update=10000, policy=policy)\n",
        "dqn.compile(Adam(lr=0.00025), metrics=['mae'])\n",
        "\n",
        "dqn.load_weights('./drive/ProX/dqn_eps2_{}_weights.h5f'.format(ENV_NAME))\n",
        "dqn.test(env, nb_episodes=5, visualize=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: Could not seed environment <SuperMarioBrosEnv<SuperMarioBros-v0>>\u001b[0m\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "permute_2 (Permute)          (None, 84, 84, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 20, 20, 32)        8224      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 20, 20, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 9, 9, 64)          32832     \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 9, 9, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 7, 7, 64)          36928     \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 512)               1606144   \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 7)                 3591      \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 7)                 0         \n",
            "=================================================================\n",
            "Total params: 1,687,719\n",
            "Trainable params: 1,687,719\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: 166.000, steps: 8439\n",
            "Episode 2: reward: 166.000, steps: 8439\n",
            "Episode 3: reward: 166.000, steps: 8439\n",
            "Episode 4: reward: 166.000, steps: 8439\n",
            "Episode 5: reward: 166.000, steps: 8439\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f51e780ae48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "v1DxxfkK8YyO",
        "colab_type": "code",
        "outputId": "69ec835c-962e-46c2-9e16-5bea58e7192a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6715
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import keras\n",
        "#import pydot\n",
        "\n",
        "from PIL import Image\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Conv2D, Reshape, MaxPooling2D, Permute\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import plot_model\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy, MaxBoltzmannQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.core import Processor\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
        "\n",
        "from nes_py.wrappers import BinarySpaceToDiscreteSpaceEnv\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
        "\n",
        "from gym import wrappers\n",
        "\n",
        "from keras import backend as K # 柴田\n",
        "\n",
        "folder = './drive/ProX/mario6_log3'\n",
        "\n",
        "\n",
        "ENV_NAME = 'SuperMarioBros-v0'\n",
        "\n",
        "INPUT_SHAPE = (84, 84)\n",
        "WINDOW_LENGTH = 4\n",
        "\n",
        "class GameProcessor(Processor):\n",
        "    def process_observation(self, observation):\n",
        "        assert observation.ndim == 3\n",
        "        img = Image.fromarray(observation)\n",
        "        img = img.resize(INPUT_SHAPE).convert('L')\n",
        "        processed_observation = np.array(img)\n",
        "        assert processed_observation.shape == INPUT_SHAPE\n",
        "        return processed_observation.astype('uint8')\n",
        "\n",
        "    def process_state_batch(self, batch):\n",
        "        processed_batch = batch.astype('float32') / 255.\n",
        "        return processed_batch\n",
        "\n",
        "    def process_reward(self, reward):\n",
        "        return np.clip(reward, -1., 1.)\n",
        "\n",
        "env = gym_super_mario_bros.make(ENV_NAME)\n",
        "env = BinarySpaceToDiscreteSpaceEnv(env, SIMPLE_MOVEMENT)\n",
        "\n",
        "env = wrappers.Monitor(env, folder, force=False, video_callable=(lambda ep: ep % 10 == 0))\n",
        "\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = len(SIMPLE_MOVEMENT)\n",
        "\n",
        "#print(env.observation_space.shape) #(240, 256, 3)\n",
        "\n",
        "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
        "model = Sequential()\n",
        "model.add( Permute((2,3,1), input_shape=input_shape ) )\n",
        "model.add(Conv2D(32, (8, 8), strides=(4, 4)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (4, 4), strides=(2, 2)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3), strides=(1, 1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))\n",
        "print(model.summary())\n",
        "\n",
        "#plot_model(model, to_file='./drive/ProX/model.png')\n",
        "\n",
        "memory = SequentialMemory(limit=300000, window_length=WINDOW_LENGTH)\n",
        "processor = GameProcessor()\n",
        "policy = MaxBoltzmannQPolicy(eps=0.8)\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=500, \n",
        "             processor=processor, target_model_update=10000, policy=policy)\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "dqn.load_weights('./drive/ProX/dqn_epsbol4_{}_weights.h5f'.format(ENV_NAME))\n",
        "checkpoint_weights_filename = folder + '/dqn_' + ENV_NAME + '_weights_{step}.h5f'\n",
        "log_filename = folder + '/dqn_{}_log.json'.format(ENV_NAME)\n",
        "\n",
        "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000)]\n",
        "callbacks += [FileLogger(log_filename, interval=100)]\n",
        "dqn.fit(env,callbacks=callbacks, nb_steps=300000, visualize=False, verbose=2)\n",
        "dqn.save_weights('./drive/ProX/dqn_epsbol4_{}_weights.h5f'.format(ENV_NAME), overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: Could not seed environment <SuperMarioBrosEnv<SuperMarioBros-v0>>\u001b[0m\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "permute_2 (Permute)          (None, 84, 84, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 20, 20, 32)        8224      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 20, 20, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 9, 9, 64)          32832     \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 9, 9, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 7, 7, 64)          36928     \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 512)               1606144   \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 7)                 3591      \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 7)                 0         \n",
            "=================================================================\n",
            "Total params: 1,687,719\n",
            "Trainable params: 1,687,719\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Training for 300000 steps ...\n",
            "    598/300000: episode: 1, duration: 24.546s, episode steps: 598, steps per second: 24, episode reward: 322.000, mean reward: 0.538 [-1.000, 1.000], mean action: 2.885 [0.000, 6.000], mean observation: 138.213 [0.000, 252.000], loss: 2.747380, mean_absolute_error: 27.114497, mean_q: 33.263111\n",
            "   1186/300000: episode: 2, duration: 27.355s, episode steps: 588, steps per second: 21, episode reward: 277.000, mean reward: 0.471 [-1.000, 1.000], mean action: 2.741 [0.000, 6.000], mean observation: 137.952 [0.000, 252.000], loss: 0.873177, mean_absolute_error: 26.634211, mean_q: 32.990837\n",
            "   1802/300000: episode: 3, duration: 24.538s, episode steps: 616, steps per second: 25, episode reward: 318.000, mean reward: 0.516 [-1.000, 1.000], mean action: 2.899 [0.000, 6.000], mean observation: 138.179 [0.000, 252.000], loss: 0.838479, mean_absolute_error: 26.558058, mean_q: 32.963776\n",
            "   2196/300000: episode: 4, duration: 15.773s, episode steps: 394, steps per second: 25, episode reward: 276.000, mean reward: 0.701 [-1.000, 1.000], mean action: 3.071 [0.000, 6.000], mean observation: 138.647 [0.000, 252.000], loss: 0.990817, mean_absolute_error: 26.528906, mean_q: 32.864094\n",
            "   2921/300000: episode: 5, duration: 28.799s, episode steps: 725, steps per second: 25, episode reward: 291.000, mean reward: 0.401 [-1.000, 1.000], mean action: 2.557 [0.000, 6.000], mean observation: 137.968 [0.000, 252.000], loss: 0.791710, mean_absolute_error: 26.425480, mean_q: 32.774734\n",
            "   3950/300000: episode: 6, duration: 40.866s, episode steps: 1029, steps per second: 25, episode reward: 388.000, mean reward: 0.377 [-1.000, 1.000], mean action: 2.690 [0.000, 6.000], mean observation: 138.076 [0.000, 252.000], loss: 0.922757, mean_absolute_error: 26.540117, mean_q: 32.795971\n",
            "   4391/300000: episode: 7, duration: 17.744s, episode steps: 441, steps per second: 25, episode reward: 291.000, mean reward: 0.660 [-1.000, 1.000], mean action: 2.825 [0.000, 6.000], mean observation: 138.195 [0.000, 252.000], loss: 1.195408, mean_absolute_error: 26.477718, mean_q: 32.686237\n",
            "   4679/300000: episode: 8, duration: 11.576s, episode steps: 288, steps per second: 25, episode reward: 189.000, mean reward: 0.656 [-1.000, 1.000], mean action: 2.861 [0.000, 6.000], mean observation: 138.702 [0.000, 252.000], loss: 1.123811, mean_absolute_error: 26.447966, mean_q: 32.637417\n",
            "   5118/300000: episode: 9, duration: 17.714s, episode steps: 439, steps per second: 25, episode reward: 273.000, mean reward: 0.622 [-1.000, 1.000], mean action: 2.982 [0.000, 6.000], mean observation: 137.636 [0.000, 252.000], loss: 1.172929, mean_absolute_error: 26.398006, mean_q: 32.567787\n",
            "   5830/300000: episode: 10, duration: 28.628s, episode steps: 712, steps per second: 25, episode reward: 352.000, mean reward: 0.494 [-1.000, 1.000], mean action: 2.851 [0.000, 6.000], mean observation: 138.187 [0.000, 252.000], loss: 1.259128, mean_absolute_error: 26.301083, mean_q: 32.557926\n",
            "   6505/300000: episode: 11, duration: 36.543s, episode steps: 675, steps per second: 18, episode reward: 242.000, mean reward: 0.359 [-1.000, 1.000], mean action: 3.135 [0.000, 6.000], mean observation: 137.809 [0.000, 252.000], loss: 1.154300, mean_absolute_error: 26.263437, mean_q: 32.556644\n",
            "   6929/300000: episode: 12, duration: 21.314s, episode steps: 424, steps per second: 20, episode reward: 262.000, mean reward: 0.618 [-1.000, 1.000], mean action: 2.441 [0.000, 6.000], mean observation: 138.195 [0.000, 252.000], loss: 1.280308, mean_absolute_error: 26.306711, mean_q: 32.597237\n",
            "   7226/300000: episode: 13, duration: 11.971s, episode steps: 297, steps per second: 25, episode reward: 199.000, mean reward: 0.670 [-1.000, 1.000], mean action: 2.707 [0.000, 6.000], mean observation: 138.593 [0.000, 252.000], loss: 1.133778, mean_absolute_error: 26.269354, mean_q: 32.563480\n",
            "   7760/300000: episode: 14, duration: 21.559s, episode steps: 534, steps per second: 25, episode reward: 199.000, mean reward: 0.373 [-1.000, 1.000], mean action: 2.421 [0.000, 6.000], mean observation: 138.672 [0.000, 252.000], loss: 1.149279, mean_absolute_error: 26.208969, mean_q: 32.500786\n",
            "   8086/300000: episode: 15, duration: 13.203s, episode steps: 326, steps per second: 25, episode reward: 207.000, mean reward: 0.635 [-1.000, 1.000], mean action: 2.699 [0.000, 6.000], mean observation: 138.541 [0.000, 252.000], loss: 1.340229, mean_absolute_error: 26.212696, mean_q: 32.439346\n",
            "   8218/300000: episode: 16, duration: 5.437s, episode steps: 132, steps per second: 24, episode reward: 113.000, mean reward: 0.856 [-1.000, 1.000], mean action: 2.515 [0.000, 6.000], mean observation: 138.128 [0.000, 252.000], loss: 1.341303, mean_absolute_error: 26.276798, mean_q: 32.522560\n",
            "   8373/300000: episode: 17, duration: 6.360s, episode steps: 155, steps per second: 24, episode reward: 140.000, mean reward: 0.903 [-1.000, 1.000], mean action: 3.084 [0.000, 6.000], mean observation: 138.362 [0.000, 252.000], loss: 1.558076, mean_absolute_error: 26.168959, mean_q: 32.415051\n",
            "   8560/300000: episode: 18, duration: 7.613s, episode steps: 187, steps per second: 25, episode reward: 159.000, mean reward: 0.850 [-1.000, 1.000], mean action: 3.011 [0.000, 6.000], mean observation: 138.276 [0.000, 252.000], loss: 1.715020, mean_absolute_error: 26.258188, mean_q: 32.555485\n",
            "   9083/300000: episode: 19, duration: 20.981s, episode steps: 523, steps per second: 25, episode reward: 273.000, mean reward: 0.522 [-1.000, 1.000], mean action: 2.711 [0.000, 6.000], mean observation: 138.352 [0.000, 252.000], loss: 1.618740, mean_absolute_error: 26.300468, mean_q: 32.588875\n",
            "  10186/300000: episode: 20, duration: 44.343s, episode steps: 1103, steps per second: 25, episode reward: 210.000, mean reward: 0.190 [-1.000, 1.000], mean action: 2.728 [0.000, 6.000], mean observation: 137.981 [0.000, 252.000], loss: 1.682513, mean_absolute_error: 26.433775, mean_q: 32.655582\n",
            "  10745/300000: episode: 21, duration: 32.256s, episode steps: 559, steps per second: 17, episode reward: 323.000, mean reward: 0.578 [-1.000, 1.000], mean action: 2.982 [0.000, 6.000], mean observation: 138.215 [0.000, 252.000], loss: 2.675671, mean_absolute_error: 26.946985, mean_q: 32.569267\n",
            "  11264/300000: episode: 22, duration: 25.926s, episode steps: 519, steps per second: 20, episode reward: 265.000, mean reward: 0.511 [-1.000, 1.000], mean action: 2.817 [0.000, 6.000], mean observation: 138.708 [0.000, 252.000], loss: 2.029367, mean_absolute_error: 26.890690, mean_q: 32.569366\n",
            "  11654/300000: episode: 23, duration: 15.837s, episode steps: 390, steps per second: 25, episode reward: 179.000, mean reward: 0.459 [-1.000, 1.000], mean action: 2.654 [0.000, 6.000], mean observation: 137.786 [0.000, 252.000], loss: 1.851889, mean_absolute_error: 26.843555, mean_q: 32.552013\n",
            "  11974/300000: episode: 24, duration: 12.986s, episode steps: 320, steps per second: 25, episode reward: 165.000, mean reward: 0.516 [-1.000, 1.000], mean action: 2.878 [0.000, 6.000], mean observation: 139.075 [0.000, 252.000], loss: 1.690777, mean_absolute_error: 26.744986, mean_q: 32.434330\n",
            "  12260/300000: episode: 25, duration: 11.652s, episode steps: 286, steps per second: 25, episode reward: 224.000, mean reward: 0.783 [-1.000, 1.000], mean action: 2.916 [0.000, 6.000], mean observation: 138.596 [0.000, 252.000], loss: 1.753073, mean_absolute_error: 26.783371, mean_q: 32.467789\n",
            "  12771/300000: episode: 26, duration: 20.811s, episode steps: 511, steps per second: 25, episode reward: 300.000, mean reward: 0.587 [-1.000, 1.000], mean action: 2.980 [0.000, 6.000], mean observation: 138.545 [0.000, 252.000], loss: 1.677107, mean_absolute_error: 26.755697, mean_q: 32.488113\n",
            "  13101/300000: episode: 27, duration: 13.427s, episode steps: 330, steps per second: 25, episode reward: 191.000, mean reward: 0.579 [-1.000, 1.000], mean action: 2.933 [0.000, 6.000], mean observation: 138.549 [0.000, 252.000], loss: 1.704234, mean_absolute_error: 26.665602, mean_q: 32.361958\n",
            "  13455/300000: episode: 28, duration: 14.479s, episode steps: 354, steps per second: 24, episode reward: 249.000, mean reward: 0.703 [-1.000, 1.000], mean action: 2.712 [0.000, 6.000], mean observation: 137.857 [0.000, 252.000], loss: 1.645930, mean_absolute_error: 26.625309, mean_q: 32.399986\n",
            "  13850/300000: episode: 29, duration: 15.859s, episode steps: 395, steps per second: 25, episode reward: 219.000, mean reward: 0.554 [-1.000, 1.000], mean action: 2.744 [0.000, 6.000], mean observation: 138.364 [0.000, 252.000], loss: 1.712822, mean_absolute_error: 26.580862, mean_q: 32.305424\n",
            "  14283/300000: episode: 30, duration: 17.371s, episode steps: 433, steps per second: 25, episode reward: 235.000, mean reward: 0.543 [-1.000, 1.000], mean action: 2.702 [0.000, 6.000], mean observation: 138.344 [0.000, 252.000], loss: 1.626324, mean_absolute_error: 26.551439, mean_q: 32.262375\n",
            "  14682/300000: episode: 31, duration: 24.921s, episode steps: 399, steps per second: 16, episode reward: 156.000, mean reward: 0.391 [-1.000, 1.000], mean action: 2.649 [0.000, 6.000], mean observation: 138.090 [0.000, 252.000], loss: 1.682568, mean_absolute_error: 26.549082, mean_q: 32.297119\n",
            "  15330/300000: episode: 32, duration: 30.211s, episode steps: 648, steps per second: 21, episode reward: 148.000, mean reward: 0.228 [-1.000, 1.000], mean action: 2.880 [0.000, 6.000], mean observation: 137.820 [0.000, 252.000], loss: 1.592081, mean_absolute_error: 26.515764, mean_q: 32.223251\n",
            "  15735/300000: episode: 33, duration: 16.214s, episode steps: 405, steps per second: 25, episode reward: 196.000, mean reward: 0.484 [-1.000, 1.000], mean action: 2.506 [0.000, 6.000], mean observation: 137.607 [0.000, 252.000], loss: 1.549062, mean_absolute_error: 26.474438, mean_q: 32.177128\n",
            "  17710/300000: episode: 34, duration: 78.244s, episode steps: 1975, steps per second: 25, episode reward: 333.000, mean reward: 0.169 [-1.000, 1.000], mean action: 2.858 [0.000, 6.000], mean observation: 135.834 [0.000, 252.000], loss: 1.545820, mean_absolute_error: 26.451578, mean_q: 32.145714\n",
            "  17964/300000: episode: 35, duration: 10.187s, episode steps: 254, steps per second: 25, episode reward: 151.000, mean reward: 0.594 [-1.000, 1.000], mean action: 2.732 [0.000, 6.000], mean observation: 138.784 [0.000, 252.000], loss: 1.553718, mean_absolute_error: 26.337915, mean_q: 31.998758\n",
            "  18373/300000: episode: 36, duration: 16.359s, episode steps: 409, steps per second: 25, episode reward: 202.000, mean reward: 0.494 [-1.000, 1.000], mean action: 2.905 [0.000, 6.000], mean observation: 138.208 [0.000, 252.000], loss: 1.506111, mean_absolute_error: 26.378660, mean_q: 32.004944\n",
            "  18696/300000: episode: 37, duration: 13.018s, episode steps: 323, steps per second: 25, episode reward: 205.000, mean reward: 0.635 [-1.000, 1.000], mean action: 2.684 [0.000, 6.000], mean observation: 137.928 [0.000, 252.000], loss: 1.603840, mean_absolute_error: 26.375668, mean_q: 31.971375\n",
            "  19530/300000: episode: 38, duration: 33.088s, episode steps: 834, steps per second: 25, episode reward: 217.000, mean reward: 0.260 [-1.000, 1.000], mean action: 2.872 [0.000, 6.000], mean observation: 137.933 [0.000, 252.000], loss: 1.582587, mean_absolute_error: 26.316252, mean_q: 31.973335\n",
            "  19771/300000: episode: 39, duration: 9.695s, episode steps: 241, steps per second: 25, episode reward: 210.000, mean reward: 0.871 [-1.000, 1.000], mean action: 2.560 [0.000, 6.000], mean observation: 138.516 [0.000, 252.000], loss: 1.358544, mean_absolute_error: 26.359756, mean_q: 31.979387\n",
            "  20520/300000: episode: 40, duration: 29.877s, episode steps: 749, steps per second: 25, episode reward: 393.000, mean reward: 0.525 [-1.000, 1.000], mean action: 2.702 [0.000, 6.000], mean observation: 137.967 [0.000, 252.000], loss: 2.137357, mean_absolute_error: 26.667398, mean_q: 32.040928\n",
            "  20766/300000: episode: 41, duration: 18.496s, episode steps: 246, steps per second: 13, episode reward: 171.000, mean reward: 0.695 [-1.000, 1.000], mean action: 3.000 [0.000, 6.000], mean observation: 138.538 [0.000, 252.000], loss: 2.078368, mean_absolute_error: 26.722643, mean_q: 31.941000\n",
            "  21065/300000: episode: 42, duration: 15.992s, episode steps: 299, steps per second: 19, episode reward: 222.000, mean reward: 0.742 [-1.000, 1.000], mean action: 2.462 [0.000, 6.000], mean observation: 138.702 [0.000, 252.000], loss: 2.008703, mean_absolute_error: 26.660570, mean_q: 31.889355\n",
            "  21447/300000: episode: 43, duration: 15.233s, episode steps: 382, steps per second: 25, episode reward: 194.000, mean reward: 0.508 [-1.000, 1.000], mean action: 2.783 [0.000, 6.000], mean observation: 138.455 [0.000, 252.000], loss: 2.009814, mean_absolute_error: 26.762465, mean_q: 31.970097\n",
            "  21998/300000: episode: 44, duration: 21.988s, episode steps: 551, steps per second: 25, episode reward: 216.000, mean reward: 0.392 [-1.000, 1.000], mean action: 2.365 [0.000, 6.000], mean observation: 138.341 [0.000, 252.000], loss: 1.790064, mean_absolute_error: 26.701929, mean_q: 31.964001\n",
            "  22812/300000: episode: 45, duration: 32.617s, episode steps: 814, steps per second: 25, episode reward: 267.000, mean reward: 0.328 [-1.000, 1.000], mean action: 2.555 [0.000, 6.000], mean observation: 138.242 [0.000, 252.000], loss: 1.528315, mean_absolute_error: 26.628448, mean_q: 31.831062\n",
            "  23142/300000: episode: 46, duration: 13.505s, episode steps: 330, steps per second: 24, episode reward: 215.000, mean reward: 0.652 [-1.000, 1.000], mean action: 2.927 [0.000, 6.000], mean observation: 138.350 [0.000, 252.000], loss: 1.626032, mean_absolute_error: 26.591324, mean_q: 31.808043\n",
            "  23648/300000: episode: 47, duration: 20.434s, episode steps: 506, steps per second: 25, episode reward: 359.000, mean reward: 0.709 [-1.000, 1.000], mean action: 2.660 [0.000, 6.000], mean observation: 138.076 [0.000, 252.000], loss: 1.596976, mean_absolute_error: 26.584230, mean_q: 31.829679\n",
            "  24261/300000: episode: 48, duration: 24.769s, episode steps: 613, steps per second: 25, episode reward: 227.000, mean reward: 0.370 [-1.000, 1.000], mean action: 2.538 [0.000, 6.000], mean observation: 138.106 [0.000, 252.000], loss: 1.550071, mean_absolute_error: 26.583971, mean_q: 31.849720\n",
            "  24707/300000: episode: 49, duration: 17.993s, episode steps: 446, steps per second: 25, episode reward: 246.000, mean reward: 0.552 [-1.000, 1.000], mean action: 2.989 [0.000, 6.000], mean observation: 138.742 [0.000, 252.000], loss: 1.567584, mean_absolute_error: 26.621956, mean_q: 31.861643\n",
            "  24986/300000: episode: 50, duration: 11.245s, episode steps: 279, steps per second: 25, episode reward: 181.000, mean reward: 0.649 [-1.000, 1.000], mean action: 2.907 [0.000, 6.000], mean observation: 138.734 [0.000, 252.000], loss: 1.345535, mean_absolute_error: 26.567991, mean_q: 31.798094\n",
            "  25324/300000: episode: 51, duration: 29.264s, episode steps: 338, steps per second: 12, episode reward: 210.000, mean reward: 0.621 [-1.000, 1.000], mean action: 2.950 [0.000, 6.000], mean observation: 138.324 [0.000, 252.000], loss: 1.558033, mean_absolute_error: 26.556213, mean_q: 31.792824\n",
            "  25912/300000: episode: 52, duration: 28.126s, episode steps: 588, steps per second: 21, episode reward: 303.000, mean reward: 0.515 [-1.000, 1.000], mean action: 2.685 [0.000, 6.000], mean observation: 138.495 [0.000, 252.000], loss: 1.496263, mean_absolute_error: 26.519527, mean_q: 31.733976\n",
            "  26152/300000: episode: 53, duration: 9.782s, episode steps: 240, steps per second: 25, episode reward: 190.000, mean reward: 0.792 [-1.000, 1.000], mean action: 2.788 [0.000, 6.000], mean observation: 138.752 [0.000, 252.000], loss: 1.392864, mean_absolute_error: 26.536974, mean_q: 31.756489\n",
            "  26789/300000: episode: 54, duration: 25.566s, episode steps: 637, steps per second: 25, episode reward: 259.000, mean reward: 0.407 [-1.000, 1.000], mean action: 2.490 [0.000, 6.000], mean observation: 138.394 [0.000, 252.000], loss: 1.491732, mean_absolute_error: 26.505402, mean_q: 31.718088\n",
            "  26972/300000: episode: 55, duration: 7.434s, episode steps: 183, steps per second: 25, episode reward: 168.000, mean reward: 0.918 [-1.000, 1.000], mean action: 2.973 [0.000, 6.000], mean observation: 138.455 [0.000, 252.000], loss: 1.621959, mean_absolute_error: 26.554180, mean_q: 31.757055\n",
            "  27606/300000: episode: 56, duration: 25.291s, episode steps: 634, steps per second: 25, episode reward: 241.000, mean reward: 0.380 [-1.000, 1.000], mean action: 2.569 [0.000, 6.000], mean observation: 137.781 [0.000, 252.000], loss: 1.421388, mean_absolute_error: 26.589052, mean_q: 31.823999\n",
            "  27882/300000: episode: 57, duration: 11.191s, episode steps: 276, steps per second: 25, episode reward: 202.000, mean reward: 0.732 [-1.000, 1.000], mean action: 2.482 [0.000, 6.000], mean observation: 138.044 [0.000, 252.000], loss: 1.427201, mean_absolute_error: 26.594706, mean_q: 31.801426\n",
            "  28380/300000: episode: 58, duration: 19.852s, episode steps: 498, steps per second: 25, episode reward: 241.000, mean reward: 0.484 [-1.000, 1.000], mean action: 2.478 [0.000, 6.000], mean observation: 138.179 [0.000, 252.000], loss: 1.652823, mean_absolute_error: 26.517601, mean_q: 31.730534\n",
            "  28910/300000: episode: 59, duration: 21.119s, episode steps: 530, steps per second: 25, episode reward: 327.000, mean reward: 0.617 [-1.000, 1.000], mean action: 2.706 [0.000, 6.000], mean observation: 138.268 [0.000, 252.000], loss: 1.345326, mean_absolute_error: 26.545431, mean_q: 31.753036\n",
            "  29237/300000: episode: 60, duration: 13.366s, episode steps: 327, steps per second: 24, episode reward: 137.000, mean reward: 0.419 [-1.000, 1.000], mean action: 2.777 [0.000, 6.000], mean observation: 138.173 [0.000, 252.000], loss: 1.658274, mean_absolute_error: 26.559986, mean_q: 31.755293\n",
            "  29721/300000: episode: 61, duration: 28.760s, episode steps: 484, steps per second: 17, episode reward: 327.000, mean reward: 0.676 [-1.000, 1.000], mean action: 2.868 [0.000, 6.000], mean observation: 138.456 [0.000, 252.000], loss: 1.828252, mean_absolute_error: 26.517023, mean_q: 31.705151\n",
            "  30006/300000: episode: 62, duration: 15.270s, episode steps: 285, steps per second: 19, episode reward: 209.000, mean reward: 0.733 [-1.000, 1.000], mean action: 2.477 [0.000, 6.000], mean observation: 138.611 [0.000, 252.000], loss: 1.617829, mean_absolute_error: 26.583889, mean_q: 31.756887\n",
            "  30908/300000: episode: 63, duration: 36.378s, episode steps: 902, steps per second: 25, episode reward: 202.000, mean reward: 0.224 [-1.000, 1.000], mean action: 2.662 [0.000, 6.000], mean observation: 138.053 [0.000, 252.000], loss: 1.797840, mean_absolute_error: 26.743799, mean_q: 31.841791\n",
            "  31163/300000: episode: 64, duration: 10.415s, episode steps: 255, steps per second: 24, episode reward: 200.000, mean reward: 0.784 [-1.000, 1.000], mean action: 2.471 [0.000, 6.000], mean observation: 138.647 [0.000, 252.000], loss: 1.577238, mean_absolute_error: 26.667885, mean_q: 31.687637\n",
            "  31580/300000: episode: 65, duration: 16.751s, episode steps: 417, steps per second: 25, episode reward: 301.000, mean reward: 0.722 [-1.000, 1.000], mean action: 2.549 [0.000, 6.000], mean observation: 138.603 [0.000, 252.000], loss: 1.532774, mean_absolute_error: 26.689589, mean_q: 31.704830\n",
            "  31877/300000: episode: 66, duration: 12.062s, episode steps: 297, steps per second: 25, episode reward: 238.000, mean reward: 0.801 [-1.000, 1.000], mean action: 2.646 [0.000, 6.000], mean observation: 138.806 [0.000, 252.000], loss: 1.430359, mean_absolute_error: 26.685858, mean_q: 31.691519\n",
            "  32379/300000: episode: 67, duration: 20.149s, episode steps: 502, steps per second: 25, episode reward: 325.000, mean reward: 0.647 [-1.000, 1.000], mean action: 2.659 [0.000, 6.000], mean observation: 138.349 [0.000, 252.000], loss: 1.578690, mean_absolute_error: 26.650555, mean_q: 31.709412\n",
            "  32728/300000: episode: 68, duration: 14.055s, episode steps: 349, steps per second: 25, episode reward: 221.000, mean reward: 0.633 [-1.000, 1.000], mean action: 2.862 [0.000, 6.000], mean observation: 138.656 [0.000, 252.000], loss: 1.324732, mean_absolute_error: 26.691000, mean_q: 31.739061\n",
            "  33072/300000: episode: 69, duration: 13.814s, episode steps: 344, steps per second: 25, episode reward: 200.000, mean reward: 0.581 [-1.000, 1.000], mean action: 2.561 [0.000, 6.000], mean observation: 138.447 [0.000, 252.000], loss: 1.719119, mean_absolute_error: 26.716215, mean_q: 31.804319\n",
            "  33627/300000: episode: 70, duration: 22.624s, episode steps: 555, steps per second: 25, episode reward: 266.000, mean reward: 0.479 [-1.000, 1.000], mean action: 2.555 [0.000, 6.000], mean observation: 138.126 [0.000, 252.000], loss: 1.398706, mean_absolute_error: 26.699532, mean_q: 31.759277\n",
            "  33884/300000: episode: 71, duration: 18.865s, episode steps: 257, steps per second: 14, episode reward: 210.000, mean reward: 0.817 [-1.000, 1.000], mean action: 2.716 [0.000, 6.000], mean observation: 138.090 [0.000, 252.000], loss: 1.446510, mean_absolute_error: 26.704813, mean_q: 31.781828\n",
            "  33986/300000: episode: 72, duration: 8.339s, episode steps: 102, steps per second: 12, episode reward: 88.000, mean reward: 0.863 [-1.000, 1.000], mean action: 2.902 [0.000, 6.000], mean observation: 137.941 [0.000, 252.000], loss: 1.454839, mean_absolute_error: 26.588537, mean_q: 31.743923\n",
            "  34330/300000: episode: 73, duration: 13.791s, episode steps: 344, steps per second: 25, episode reward: 228.000, mean reward: 0.663 [-1.000, 1.000], mean action: 2.703 [0.000, 6.000], mean observation: 138.546 [0.000, 252.000], loss: 1.466692, mean_absolute_error: 26.705112, mean_q: 31.785524\n",
            "  34691/300000: episode: 74, duration: 14.489s, episode steps: 361, steps per second: 25, episode reward: 248.000, mean reward: 0.687 [-1.000, 1.000], mean action: 2.909 [0.000, 6.000], mean observation: 138.492 [0.000, 252.000], loss: 1.309410, mean_absolute_error: 26.623072, mean_q: 31.706236\n",
            "  34947/300000: episode: 75, duration: 10.322s, episode steps: 256, steps per second: 25, episode reward: 214.000, mean reward: 0.836 [-1.000, 1.000], mean action: 2.793 [0.000, 6.000], mean observation: 138.627 [0.000, 252.000], loss: 1.350737, mean_absolute_error: 26.669353, mean_q: 31.724821\n",
            "  35246/300000: episode: 76, duration: 11.963s, episode steps: 299, steps per second: 25, episode reward: 216.000, mean reward: 0.722 [-1.000, 1.000], mean action: 2.789 [0.000, 6.000], mean observation: 138.431 [0.000, 252.000], loss: 1.337100, mean_absolute_error: 26.587557, mean_q: 31.653872\n",
            "  35644/300000: episode: 77, duration: 15.998s, episode steps: 398, steps per second: 25, episode reward: 242.000, mean reward: 0.608 [-1.000, 1.000], mean action: 2.590 [0.000, 6.000], mean observation: 138.516 [0.000, 252.000], loss: 1.592577, mean_absolute_error: 26.582830, mean_q: 31.660583\n",
            "  36125/300000: episode: 78, duration: 19.318s, episode steps: 481, steps per second: 25, episode reward: 278.000, mean reward: 0.578 [-1.000, 1.000], mean action: 2.786 [0.000, 6.000], mean observation: 138.743 [0.000, 252.000], loss: 1.406209, mean_absolute_error: 26.636684, mean_q: 31.717545\n",
            "  36628/300000: episode: 79, duration: 20.470s, episode steps: 503, steps per second: 25, episode reward: 340.000, mean reward: 0.676 [-1.000, 1.000], mean action: 2.598 [0.000, 6.000], mean observation: 138.321 [0.000, 252.000], loss: 1.247197, mean_absolute_error: 26.668259, mean_q: 31.731400\n",
            "  37017/300000: episode: 80, duration: 15.618s, episode steps: 389, steps per second: 25, episode reward: 250.000, mean reward: 0.643 [-1.000, 1.000], mean action: 2.347 [0.000, 6.000], mean observation: 138.296 [0.000, 252.000], loss: 1.336175, mean_absolute_error: 26.642992, mean_q: 31.686325\n",
            "  37511/300000: episode: 81, duration: 28.785s, episode steps: 494, steps per second: 17, episode reward: 284.000, mean reward: 0.575 [-1.000, 1.000], mean action: 2.931 [0.000, 6.000], mean observation: 138.528 [0.000, 252.000], loss: 1.518060, mean_absolute_error: 26.673872, mean_q: 31.761524\n",
            "  37910/300000: episode: 82, duration: 20.010s, episode steps: 399, steps per second: 20, episode reward: 256.000, mean reward: 0.642 [-1.000, 1.000], mean action: 2.677 [0.000, 6.000], mean observation: 138.874 [0.000, 252.000], loss: 1.540351, mean_absolute_error: 26.634314, mean_q: 31.677259\n",
            "  38502/300000: episode: 83, duration: 23.956s, episode steps: 592, steps per second: 25, episode reward: 306.000, mean reward: 0.517 [-1.000, 1.000], mean action: 2.735 [0.000, 6.000], mean observation: 138.162 [0.000, 252.000], loss: 1.445861, mean_absolute_error: 26.561449, mean_q: 31.627007\n",
            "  38889/300000: episode: 84, duration: 15.776s, episode steps: 387, steps per second: 25, episode reward: 203.000, mean reward: 0.525 [-1.000, 1.000], mean action: 2.602 [0.000, 6.000], mean observation: 137.824 [0.000, 252.000], loss: 1.460991, mean_absolute_error: 26.575672, mean_q: 31.651205\n",
            "  39326/300000: episode: 85, duration: 17.656s, episode steps: 437, steps per second: 25, episode reward: 250.000, mean reward: 0.572 [-1.000, 1.000], mean action: 2.735 [0.000, 6.000], mean observation: 138.738 [0.000, 252.000], loss: 1.368713, mean_absolute_error: 26.519714, mean_q: 31.568346\n",
            "  39711/300000: episode: 86, duration: 15.556s, episode steps: 385, steps per second: 25, episode reward: 246.000, mean reward: 0.639 [-1.000, 1.000], mean action: 2.499 [0.000, 6.000], mean observation: 137.959 [0.000, 252.000], loss: 1.332051, mean_absolute_error: 26.516287, mean_q: 31.545338\n",
            "  39954/300000: episode: 87, duration: 9.943s, episode steps: 243, steps per second: 24, episode reward: 196.000, mean reward: 0.807 [-1.000, 1.000], mean action: 2.745 [0.000, 6.000], mean observation: 137.850 [0.000, 252.000], loss: 1.576676, mean_absolute_error: 26.578938, mean_q: 31.624949\n",
            "  40265/300000: episode: 88, duration: 12.838s, episode steps: 311, steps per second: 24, episode reward: 241.000, mean reward: 0.775 [-1.000, 1.000], mean action: 2.518 [0.000, 6.000], mean observation: 137.776 [0.000, 252.000], loss: 1.966264, mean_absolute_error: 26.696226, mean_q: 31.734953\n",
            "  40915/300000: episode: 89, duration: 26.159s, episode steps: 650, steps per second: 25, episode reward: 260.000, mean reward: 0.400 [-1.000, 1.000], mean action: 2.749 [0.000, 6.000], mean observation: 138.259 [0.000, 252.000], loss: 1.620144, mean_absolute_error: 26.706207, mean_q: 31.673969\n",
            "  41146/300000: episode: 90, duration: 9.379s, episode steps: 231, steps per second: 25, episode reward: 200.000, mean reward: 0.866 [-1.000, 1.000], mean action: 2.827 [0.000, 6.000], mean observation: 138.431 [0.000, 252.000], loss: 1.736080, mean_absolute_error: 26.760344, mean_q: 31.750492\n",
            "  41491/300000: episode: 91, duration: 22.288s, episode steps: 345, steps per second: 15, episode reward: 266.000, mean reward: 0.771 [-1.000, 1.000], mean action: 2.875 [0.000, 6.000], mean observation: 137.933 [0.000, 252.000], loss: 1.582812, mean_absolute_error: 26.691933, mean_q: 31.661682\n",
            "  42018/300000: episode: 92, duration: 25.669s, episode steps: 527, steps per second: 21, episode reward: 304.000, mean reward: 0.577 [-1.000, 1.000], mean action: 2.945 [0.000, 6.000], mean observation: 138.268 [0.000, 252.000], loss: 1.446161, mean_absolute_error: 26.682699, mean_q: 31.663704\n",
            "  42626/300000: episode: 93, duration: 24.671s, episode steps: 608, steps per second: 25, episode reward: 282.000, mean reward: 0.464 [-1.000, 1.000], mean action: 2.891 [0.000, 6.000], mean observation: 138.250 [0.000, 252.000], loss: 1.435123, mean_absolute_error: 26.660376, mean_q: 31.633808\n",
            "  42882/300000: episode: 94, duration: 10.549s, episode steps: 256, steps per second: 24, episode reward: 190.000, mean reward: 0.742 [-1.000, 1.000], mean action: 2.934 [0.000, 6.000], mean observation: 138.283 [0.000, 252.000], loss: 1.379365, mean_absolute_error: 26.705585, mean_q: 31.682606\n",
            "  43098/300000: episode: 95, duration: 8.777s, episode steps: 216, steps per second: 25, episode reward: 111.000, mean reward: 0.514 [-1.000, 1.000], mean action: 2.995 [0.000, 6.000], mean observation: 138.989 [0.000, 252.000], loss: 1.411259, mean_absolute_error: 26.635593, mean_q: 31.626554\n",
            "  43424/300000: episode: 96, duration: 13.355s, episode steps: 326, steps per second: 24, episode reward: 226.000, mean reward: 0.693 [-1.000, 1.000], mean action: 2.356 [0.000, 6.000], mean observation: 137.799 [0.000, 252.000], loss: 1.407411, mean_absolute_error: 26.698435, mean_q: 31.677069\n",
            "  43970/300000: episode: 97, duration: 21.988s, episode steps: 546, steps per second: 25, episode reward: 284.000, mean reward: 0.520 [-1.000, 1.000], mean action: 2.668 [0.000, 6.000], mean observation: 138.229 [0.000, 252.000], loss: 1.297830, mean_absolute_error: 26.653162, mean_q: 31.610577\n",
            "  44209/300000: episode: 98, duration: 9.804s, episode steps: 239, steps per second: 24, episode reward: 151.000, mean reward: 0.632 [-1.000, 1.000], mean action: 3.046 [0.000, 6.000], mean observation: 138.281 [0.000, 252.000], loss: 1.252407, mean_absolute_error: 26.696615, mean_q: 31.671864\n",
            "  44686/300000: episode: 99, duration: 19.346s, episode steps: 477, steps per second: 25, episode reward: 221.000, mean reward: 0.463 [-1.000, 1.000], mean action: 2.795 [0.000, 6.000], mean observation: 138.110 [0.000, 252.000], loss: 1.305370, mean_absolute_error: 26.682295, mean_q: 31.657539\n",
            "  44996/300000: episode: 100, duration: 12.585s, episode steps: 310, steps per second: 25, episode reward: 213.000, mean reward: 0.687 [-1.000, 1.000], mean action: 2.984 [0.000, 6.000], mean observation: 138.131 [0.000, 252.000], loss: 1.198443, mean_absolute_error: 26.617748, mean_q: 31.619030\n",
            "  45703/300000: episode: 101, duration: 39.932s, episode steps: 707, steps per second: 18, episode reward: 310.000, mean reward: 0.438 [-1.000, 1.000], mean action: 2.754 [0.000, 6.000], mean observation: 138.103 [0.000, 252.000], loss: 1.432101, mean_absolute_error: 26.708578, mean_q: 31.680481\n",
            "  46056/300000: episode: 102, duration: 18.682s, episode steps: 353, steps per second: 19, episode reward: 248.000, mean reward: 0.703 [-1.000, 1.000], mean action: 2.915 [0.000, 6.000], mean observation: 138.555 [0.000, 252.000], loss: 1.209821, mean_absolute_error: 26.682985, mean_q: 31.638102\n",
            "  46710/300000: episode: 103, duration: 26.387s, episode steps: 654, steps per second: 25, episode reward: 144.000, mean reward: 0.220 [-1.000, 1.000], mean action: 2.584 [0.000, 6.000], mean observation: 137.481 [0.000, 252.000], loss: 1.190114, mean_absolute_error: 26.714653, mean_q: 31.698837\n",
            "  47202/300000: episode: 104, duration: 19.871s, episode steps: 492, steps per second: 25, episode reward: 325.000, mean reward: 0.661 [-1.000, 1.000], mean action: 2.699 [0.000, 6.000], mean observation: 138.343 [0.000, 252.000], loss: 1.233033, mean_absolute_error: 26.699953, mean_q: 31.675238\n",
            "  47678/300000: episode: 105, duration: 19.203s, episode steps: 476, steps per second: 25, episode reward: 270.000, mean reward: 0.567 [-1.000, 1.000], mean action: 2.468 [0.000, 6.000], mean observation: 137.728 [0.000, 252.000], loss: 1.359919, mean_absolute_error: 26.784225, mean_q: 31.817762\n",
            "  48072/300000: episode: 106, duration: 15.888s, episode steps: 394, steps per second: 25, episode reward: 257.000, mean reward: 0.652 [-1.000, 1.000], mean action: 2.926 [0.000, 6.000], mean observation: 138.018 [0.000, 252.000], loss: 1.197196, mean_absolute_error: 26.763414, mean_q: 31.772213\n",
            "  48591/300000: episode: 107, duration: 21.027s, episode steps: 519, steps per second: 25, episode reward: 317.000, mean reward: 0.611 [-1.000, 1.000], mean action: 2.960 [0.000, 6.000], mean observation: 138.291 [0.000, 252.000], loss: 1.204128, mean_absolute_error: 26.854050, mean_q: 31.856997\n",
            "  48853/300000: episode: 108, duration: 10.717s, episode steps: 262, steps per second: 24, episode reward: 187.000, mean reward: 0.714 [-1.000, 1.000], mean action: 2.450 [0.000, 6.000], mean observation: 138.178 [0.000, 252.000], loss: 1.716108, mean_absolute_error: 26.762251, mean_q: 31.783688\n",
            "  49260/300000: episode: 109, duration: 16.544s, episode steps: 407, steps per second: 25, episode reward: 212.000, mean reward: 0.521 [-1.000, 1.000], mean action: 2.683 [0.000, 6.000], mean observation: 138.554 [0.000, 252.000], loss: 1.088089, mean_absolute_error: 26.846420, mean_q: 31.870892\n",
            "  49692/300000: episode: 110, duration: 17.460s, episode steps: 432, steps per second: 25, episode reward: 362.000, mean reward: 0.838 [-1.000, 1.000], mean action: 2.611 [0.000, 6.000], mean observation: 138.172 [0.000, 252.000], loss: 1.258270, mean_absolute_error: 26.868778, mean_q: 31.886812\n",
            "  50286/300000: episode: 111, duration: 33.786s, episode steps: 594, steps per second: 18, episode reward: 326.000, mean reward: 0.549 [-1.000, 1.000], mean action: 2.904 [0.000, 6.000], mean observation: 138.352 [0.000, 252.000], loss: 1.334656, mean_absolute_error: 26.889666, mean_q: 31.888853\n",
            "  50692/300000: episode: 112, duration: 20.446s, episode steps: 406, steps per second: 20, episode reward: 290.000, mean reward: 0.714 [-1.000, 1.000], mean action: 2.685 [0.000, 6.000], mean observation: 138.233 [0.000, 252.000], loss: 1.449113, mean_absolute_error: 26.826611, mean_q: 31.830122\n",
            "  50929/300000: episode: 113, duration: 9.537s, episode steps: 237, steps per second: 25, episode reward: 154.000, mean reward: 0.650 [-1.000, 1.000], mean action: 2.768 [0.000, 6.000], mean observation: 138.204 [0.000, 252.000], loss: 1.281650, mean_absolute_error: 26.764162, mean_q: 31.720530\n",
            "  51253/300000: episode: 114, duration: 13.129s, episode steps: 324, steps per second: 25, episode reward: 218.000, mean reward: 0.673 [-1.000, 1.000], mean action: 2.596 [0.000, 6.000], mean observation: 138.257 [0.000, 252.000], loss: 1.446007, mean_absolute_error: 26.807789, mean_q: 31.821415\n",
            "  51479/300000: episode: 115, duration: 9.253s, episode steps: 226, steps per second: 24, episode reward: 153.000, mean reward: 0.677 [-1.000, 1.000], mean action: 2.938 [0.000, 6.000], mean observation: 138.192 [0.000, 252.000], loss: 1.550246, mean_absolute_error: 26.799934, mean_q: 31.777372\n",
            "  51965/300000: episode: 116, duration: 19.613s, episode steps: 486, steps per second: 25, episode reward: 160.000, mean reward: 0.329 [-1.000, 1.000], mean action: 2.317 [0.000, 6.000], mean observation: 138.020 [0.000, 252.000], loss: 1.269923, mean_absolute_error: 26.775908, mean_q: 31.729063\n",
            "  52578/300000: episode: 117, duration: 24.761s, episode steps: 613, steps per second: 25, episode reward: 234.000, mean reward: 0.382 [-1.000, 1.000], mean action: 2.997 [0.000, 6.000], mean observation: 138.264 [0.000, 252.000], loss: 1.223277, mean_absolute_error: 26.818336, mean_q: 31.778446\n",
            "  52868/300000: episode: 118, duration: 11.645s, episode steps: 290, steps per second: 25, episode reward: 191.000, mean reward: 0.659 [-1.000, 1.000], mean action: 2.855 [0.000, 6.000], mean observation: 138.418 [0.000, 252.000], loss: 1.311241, mean_absolute_error: 26.728483, mean_q: 31.647144\n",
            "  53315/300000: episode: 119, duration: 18.071s, episode steps: 447, steps per second: 25, episode reward: 318.000, mean reward: 0.711 [-1.000, 1.000], mean action: 2.745 [0.000, 6.000], mean observation: 138.531 [0.000, 252.000], loss: 1.262457, mean_absolute_error: 26.753761, mean_q: 31.684988\n",
            "  53545/300000: episode: 120, duration: 9.300s, episode steps: 230, steps per second: 25, episode reward: 157.000, mean reward: 0.683 [-1.000, 1.000], mean action: 2.604 [0.000, 6.000], mean observation: 138.335 [0.000, 252.000], loss: 1.230855, mean_absolute_error: 26.628294, mean_q: 31.585415\n",
            "  53970/300000: episode: 121, duration: 25.843s, episode steps: 425, steps per second: 16, episode reward: 293.000, mean reward: 0.689 [-1.000, 1.000], mean action: 2.720 [0.000, 6.000], mean observation: 138.450 [0.000, 252.000], loss: 1.268112, mean_absolute_error: 26.833099, mean_q: 31.836929\n",
            "  54257/300000: episode: 122, duration: 15.417s, episode steps: 287, steps per second: 19, episode reward: 223.000, mean reward: 0.777 [-1.000, 1.000], mean action: 3.084 [0.000, 6.000], mean observation: 138.175 [0.000, 252.000], loss: 1.230704, mean_absolute_error: 26.819618, mean_q: 31.775768\n",
            "  54879/300000: episode: 123, duration: 25.023s, episode steps: 622, steps per second: 25, episode reward: 191.000, mean reward: 0.307 [-1.000, 1.000], mean action: 2.929 [0.000, 6.000], mean observation: 138.124 [0.000, 252.000], loss: 1.250036, mean_absolute_error: 26.833712, mean_q: 31.785294\n",
            "  55197/300000: episode: 124, duration: 12.814s, episode steps: 318, steps per second: 25, episode reward: 191.000, mean reward: 0.601 [-1.000, 1.000], mean action: 2.881 [0.000, 6.000], mean observation: 138.438 [0.000, 252.000], loss: 1.190786, mean_absolute_error: 26.843487, mean_q: 31.808422\n",
            "  55535/300000: episode: 125, duration: 13.584s, episode steps: 338, steps per second: 25, episode reward: 166.000, mean reward: 0.491 [-1.000, 1.000], mean action: 2.849 [0.000, 6.000], mean observation: 138.242 [0.000, 252.000], loss: 1.056960, mean_absolute_error: 26.888660, mean_q: 31.860271\n",
            "  55951/300000: episode: 126, duration: 16.898s, episode steps: 416, steps per second: 25, episode reward: 290.000, mean reward: 0.697 [-1.000, 1.000], mean action: 2.748 [0.000, 6.000], mean observation: 138.078 [0.000, 252.000], loss: 1.294212, mean_absolute_error: 26.841280, mean_q: 31.796984\n",
            "  56335/300000: episode: 127, duration: 15.572s, episode steps: 384, steps per second: 25, episode reward: 227.000, mean reward: 0.591 [-1.000, 1.000], mean action: 2.846 [0.000, 6.000], mean observation: 138.129 [0.000, 252.000], loss: 1.082881, mean_absolute_error: 26.867096, mean_q: 31.852671\n",
            "  56805/300000: episode: 128, duration: 19.001s, episode steps: 470, steps per second: 25, episode reward: 301.000, mean reward: 0.640 [-1.000, 1.000], mean action: 2.591 [0.000, 6.000], mean observation: 138.547 [0.000, 252.000], loss: 1.204959, mean_absolute_error: 26.806774, mean_q: 31.746588\n",
            "  57315/300000: episode: 129, duration: 20.764s, episode steps: 510, steps per second: 25, episode reward: 325.000, mean reward: 0.637 [-1.000, 1.000], mean action: 3.018 [0.000, 6.000], mean observation: 138.489 [0.000, 252.000], loss: 1.170095, mean_absolute_error: 26.943567, mean_q: 31.926416\n",
            "  58032/300000: episode: 130, duration: 29.372s, episode steps: 717, steps per second: 24, episode reward: 414.000, mean reward: 0.577 [-1.000, 1.000], mean action: 2.777 [0.000, 6.000], mean observation: 138.440 [0.000, 252.000], loss: 1.199195, mean_absolute_error: 26.868887, mean_q: 31.832415\n",
            "  58361/300000: episode: 131, duration: 21.362s, episode steps: 329, steps per second: 15, episode reward: 242.000, mean reward: 0.736 [-1.000, 1.000], mean action: 3.006 [0.000, 6.000], mean observation: 138.042 [0.000, 252.000], loss: 1.435539, mean_absolute_error: 26.837997, mean_q: 31.776999\n",
            "  59762/300000: episode: 132, duration: 63.277s, episode steps: 1401, steps per second: 22, episode reward: 545.000, mean reward: 0.389 [-1.000, 1.000], mean action: 2.662 [0.000, 6.000], mean observation: 123.760 [0.000, 252.000], loss: 1.135050, mean_absolute_error: 26.837061, mean_q: 31.781673\n",
            "  60154/300000: episode: 133, duration: 15.822s, episode steps: 392, steps per second: 25, episode reward: 240.000, mean reward: 0.612 [-1.000, 1.000], mean action: 2.872 [0.000, 6.000], mean observation: 138.456 [0.000, 252.000], loss: 1.540853, mean_absolute_error: 27.065096, mean_q: 32.131691\n",
            "  60543/300000: episode: 134, duration: 15.673s, episode steps: 389, steps per second: 25, episode reward: 214.000, mean reward: 0.550 [-1.000, 1.000], mean action: 2.429 [0.000, 6.000], mean observation: 138.577 [0.000, 252.000], loss: 1.516900, mean_absolute_error: 27.401392, mean_q: 32.463818\n",
            "  60801/300000: episode: 135, duration: 10.457s, episode steps: 258, steps per second: 25, episode reward: 166.000, mean reward: 0.643 [-1.000, 1.000], mean action: 2.860 [0.000, 6.000], mean observation: 138.158 [0.000, 252.000], loss: 1.253563, mean_absolute_error: 27.446749, mean_q: 32.521099\n",
            "  61284/300000: episode: 136, duration: 19.488s, episode steps: 483, steps per second: 25, episode reward: 331.000, mean reward: 0.685 [-1.000, 1.000], mean action: 2.569 [0.000, 6.000], mean observation: 138.315 [0.000, 252.000], loss: 1.275771, mean_absolute_error: 27.453306, mean_q: 32.516899\n",
            "  61426/300000: episode: 137, duration: 5.868s, episode steps: 142, steps per second: 24, episode reward: 98.000, mean reward: 0.690 [-1.000, 1.000], mean action: 2.761 [0.000, 6.000], mean observation: 138.181 [0.000, 252.000], loss: 1.369728, mean_absolute_error: 27.466230, mean_q: 32.532013\n",
            "  61680/300000: episode: 138, duration: 10.392s, episode steps: 254, steps per second: 24, episode reward: 189.000, mean reward: 0.744 [-1.000, 1.000], mean action: 2.520 [0.000, 6.000], mean observation: 138.242 [0.000, 252.000], loss: 1.492901, mean_absolute_error: 27.396271, mean_q: 32.435726\n",
            "  62174/300000: episode: 139, duration: 19.905s, episode steps: 494, steps per second: 25, episode reward: 206.000, mean reward: 0.417 [-1.000, 1.000], mean action: 2.816 [0.000, 6.000], mean observation: 138.279 [0.000, 252.000], loss: 1.383837, mean_absolute_error: 27.432108, mean_q: 32.499550\n",
            "  62582/300000: episode: 140, duration: 16.553s, episode steps: 408, steps per second: 25, episode reward: 232.000, mean reward: 0.569 [-1.000, 1.000], mean action: 2.463 [0.000, 6.000], mean observation: 137.594 [0.000, 252.000], loss: 1.620648, mean_absolute_error: 27.397394, mean_q: 32.443005\n",
            "  63070/300000: episode: 141, duration: 28.483s, episode steps: 488, steps per second: 17, episode reward: 244.000, mean reward: 0.500 [-1.000, 1.000], mean action: 2.674 [0.000, 6.000], mean observation: 138.299 [0.000, 252.000], loss: 1.511125, mean_absolute_error: 27.417715, mean_q: 32.443348\n",
            "  63334/300000: episode: 142, duration: 15.189s, episode steps: 264, steps per second: 17, episode reward: 157.000, mean reward: 0.595 [-1.000, 1.000], mean action: 3.341 [0.000, 6.000], mean observation: 138.393 [0.000, 252.000], loss: 1.552424, mean_absolute_error: 27.388275, mean_q: 32.423088\n",
            "  63586/300000: episode: 143, duration: 10.313s, episode steps: 252, steps per second: 24, episode reward: 190.000, mean reward: 0.754 [-1.000, 1.000], mean action: 2.567 [0.000, 6.000], mean observation: 138.805 [0.000, 252.000], loss: 1.345255, mean_absolute_error: 27.447895, mean_q: 32.509632\n",
            "  63760/300000: episode: 144, duration: 7.115s, episode steps: 174, steps per second: 24, episode reward: 131.000, mean reward: 0.753 [-1.000, 1.000], mean action: 2.753 [0.000, 6.000], mean observation: 138.412 [0.000, 252.000], loss: 1.336406, mean_absolute_error: 27.457430, mean_q: 32.536957\n",
            "  64248/300000: episode: 145, duration: 19.756s, episode steps: 488, steps per second: 25, episode reward: 310.000, mean reward: 0.635 [-1.000, 1.000], mean action: 2.971 [0.000, 6.000], mean observation: 138.388 [0.000, 252.000], loss: 1.362053, mean_absolute_error: 27.466724, mean_q: 32.509041\n",
            "  64626/300000: episode: 146, duration: 15.375s, episode steps: 378, steps per second: 25, episode reward: 211.000, mean reward: 0.558 [-1.000, 1.000], mean action: 2.810 [0.000, 6.000], mean observation: 138.355 [0.000, 252.000], loss: 1.254560, mean_absolute_error: 27.453751, mean_q: 32.487110\n",
            "  64918/300000: episode: 147, duration: 12.089s, episode steps: 292, steps per second: 24, episode reward: 233.000, mean reward: 0.798 [-1.000, 1.000], mean action: 2.521 [0.000, 6.000], mean observation: 138.245 [0.000, 252.000], loss: 1.355592, mean_absolute_error: 27.426140, mean_q: 32.480003\n",
            "  65277/300000: episode: 148, duration: 14.616s, episode steps: 359, steps per second: 25, episode reward: 200.000, mean reward: 0.557 [-1.000, 1.000], mean action: 2.682 [0.000, 6.000], mean observation: 137.728 [0.000, 252.000], loss: 1.297824, mean_absolute_error: 27.356106, mean_q: 32.356335\n",
            "  65614/300000: episode: 149, duration: 13.874s, episode steps: 337, steps per second: 24, episode reward: 227.000, mean reward: 0.674 [-1.000, 1.000], mean action: 2.665 [0.000, 6.000], mean observation: 138.232 [0.000, 252.000], loss: 1.176781, mean_absolute_error: 27.393702, mean_q: 32.390369\n",
            "  66172/300000: episode: 150, duration: 22.709s, episode steps: 558, steps per second: 25, episode reward: 382.000, mean reward: 0.685 [-1.000, 1.000], mean action: 2.713 [0.000, 6.000], mean observation: 138.569 [0.000, 252.000], loss: 1.601704, mean_absolute_error: 27.383783, mean_q: 32.427292\n",
            "  66559/300000: episode: 151, duration: 24.263s, episode steps: 387, steps per second: 16, episode reward: 287.000, mean reward: 0.742 [-1.000, 1.000], mean action: 2.129 [0.000, 6.000], mean observation: 138.467 [0.000, 252.000], loss: 1.523224, mean_absolute_error: 27.492718, mean_q: 32.526001\n",
            "  66832/300000: episode: 152, duration: 15.399s, episode steps: 273, steps per second: 18, episode reward: 204.000, mean reward: 0.747 [-1.000, 1.000], mean action: 3.051 [0.000, 6.000], mean observation: 138.551 [0.000, 252.000], loss: 1.400476, mean_absolute_error: 27.451284, mean_q: 32.511803\n",
            "  67396/300000: episode: 153, duration: 23.201s, episode steps: 564, steps per second: 24, episode reward: 296.000, mean reward: 0.525 [-1.000, 1.000], mean action: 2.793 [0.000, 6.000], mean observation: 138.235 [0.000, 252.000], loss: 1.360666, mean_absolute_error: 27.376593, mean_q: 32.418785\n",
            "  67560/300000: episode: 154, duration: 6.866s, episode steps: 164, steps per second: 24, episode reward: 140.000, mean reward: 0.854 [-1.000, 1.000], mean action: 2.695 [0.000, 6.000], mean observation: 138.500 [0.000, 252.000], loss: 1.175593, mean_absolute_error: 27.492224, mean_q: 32.578266\n",
            "  67791/300000: episode: 155, duration: 9.548s, episode steps: 231, steps per second: 24, episode reward: 159.000, mean reward: 0.688 [-1.000, 1.000], mean action: 2.805 [0.000, 6.000], mean observation: 138.212 [0.000, 252.000], loss: 1.127686, mean_absolute_error: 27.498169, mean_q: 32.500648\n",
            "  68654/300000: episode: 156, duration: 35.357s, episode steps: 863, steps per second: 24, episode reward: 319.000, mean reward: 0.370 [-1.000, 1.000], mean action: 2.692 [0.000, 6.000], mean observation: 137.955 [0.000, 252.000], loss: 1.501314, mean_absolute_error: 27.444620, mean_q: 32.476170\n",
            "  69079/300000: episode: 157, duration: 17.408s, episode steps: 425, steps per second: 24, episode reward: 266.000, mean reward: 0.626 [-1.000, 1.000], mean action: 2.278 [0.000, 6.000], mean observation: 138.603 [0.000, 252.000], loss: 1.259701, mean_absolute_error: 27.431150, mean_q: 32.457317\n",
            "  69526/300000: episode: 158, duration: 18.312s, episode steps: 447, steps per second: 24, episode reward: 260.000, mean reward: 0.582 [-1.000, 1.000], mean action: 3.172 [0.000, 6.000], mean observation: 138.454 [0.000, 252.000], loss: 1.324857, mean_absolute_error: 27.404652, mean_q: 32.422993\n",
            "  69769/300000: episode: 159, duration: 10.009s, episode steps: 243, steps per second: 24, episode reward: 202.000, mean reward: 0.831 [-1.000, 1.000], mean action: 2.506 [0.000, 6.000], mean observation: 138.711 [0.000, 252.000], loss: 0.840762, mean_absolute_error: 27.453764, mean_q: 32.495975\n",
            "  70058/300000: episode: 160, duration: 11.961s, episode steps: 289, steps per second: 24, episode reward: 151.000, mean reward: 0.522 [-1.000, 1.000], mean action: 2.972 [0.000, 6.000], mean observation: 138.121 [0.000, 252.000], loss: 1.405676, mean_absolute_error: 27.638500, mean_q: 32.713970\n",
            "  70476/300000: episode: 161, duration: 25.678s, episode steps: 418, steps per second: 16, episode reward: 232.000, mean reward: 0.555 [-1.000, 1.000], mean action: 2.981 [0.000, 6.000], mean observation: 138.282 [0.000, 252.000], loss: 1.624306, mean_absolute_error: 28.125103, mean_q: 33.279926\n",
            "  70884/300000: episode: 162, duration: 21.105s, episode steps: 408, steps per second: 19, episode reward: 257.000, mean reward: 0.630 [-1.000, 1.000], mean action: 2.740 [0.000, 6.000], mean observation: 138.425 [0.000, 252.000], loss: 1.525947, mean_absolute_error: 28.154282, mean_q: 33.332485\n",
            "  71264/300000: episode: 163, duration: 15.533s, episode steps: 380, steps per second: 24, episode reward: 269.000, mean reward: 0.708 [-1.000, 1.000], mean action: 2.547 [0.000, 6.000], mean observation: 138.738 [0.000, 252.000], loss: 1.856651, mean_absolute_error: 28.157375, mean_q: 33.303684\n",
            "  71603/300000: episode: 164, duration: 14.048s, episode steps: 339, steps per second: 24, episode reward: 246.000, mean reward: 0.726 [-1.000, 1.000], mean action: 2.501 [0.000, 6.000], mean observation: 138.363 [0.000, 252.000], loss: 1.584646, mean_absolute_error: 28.164560, mean_q: 33.316162\n",
            "  71803/300000: episode: 165, duration: 8.270s, episode steps: 200, steps per second: 24, episode reward: 166.000, mean reward: 0.830 [-1.000, 1.000], mean action: 2.425 [0.000, 6.000], mean observation: 138.592 [0.000, 252.000], loss: 1.545631, mean_absolute_error: 28.187538, mean_q: 33.276905\n",
            "  72043/300000: episode: 166, duration: 9.768s, episode steps: 240, steps per second: 25, episode reward: 189.000, mean reward: 0.787 [-1.000, 1.000], mean action: 2.596 [0.000, 6.000], mean observation: 138.455 [0.000, 252.000], loss: 1.334178, mean_absolute_error: 28.095730, mean_q: 33.203476\n",
            "  72385/300000: episode: 167, duration: 14.093s, episode steps: 342, steps per second: 24, episode reward: 206.000, mean reward: 0.602 [-1.000, 1.000], mean action: 2.582 [0.000, 6.000], mean observation: 138.206 [0.000, 252.000], loss: 1.432436, mean_absolute_error: 28.106745, mean_q: 33.260387\n",
            "  72984/300000: episode: 168, duration: 24.392s, episode steps: 599, steps per second: 25, episode reward: 264.000, mean reward: 0.441 [-1.000, 1.000], mean action: 2.628 [0.000, 6.000], mean observation: 138.124 [0.000, 252.000], loss: 1.604900, mean_absolute_error: 28.167200, mean_q: 33.352943\n",
            "  73307/300000: episode: 169, duration: 13.232s, episode steps: 323, steps per second: 24, episode reward: 196.000, mean reward: 0.607 [-1.000, 1.000], mean action: 2.588 [0.000, 6.000], mean observation: 138.776 [0.000, 252.000], loss: 1.832233, mean_absolute_error: 28.082697, mean_q: 33.263779\n",
            "  73698/300000: episode: 170, duration: 15.932s, episode steps: 391, steps per second: 25, episode reward: 290.000, mean reward: 0.742 [-1.000, 1.000], mean action: 2.545 [0.000, 6.000], mean observation: 138.414 [0.000, 252.000], loss: 1.479561, mean_absolute_error: 28.182665, mean_q: 33.345108\n",
            "  73950/300000: episode: 171, duration: 18.508s, episode steps: 252, steps per second: 14, episode reward: 213.000, mean reward: 0.845 [-1.000, 1.000], mean action: 2.782 [0.000, 6.000], mean observation: 138.562 [0.000, 252.000], loss: 1.728082, mean_absolute_error: 28.103977, mean_q: 33.279205\n",
            "  74254/300000: episode: 172, duration: 16.637s, episode steps: 304, steps per second: 18, episode reward: 210.000, mean reward: 0.691 [-1.000, 1.000], mean action: 2.661 [0.000, 6.000], mean observation: 138.373 [0.000, 252.000], loss: 1.484272, mean_absolute_error: 28.201645, mean_q: 33.340813\n",
            "  74752/300000: episode: 173, duration: 20.253s, episode steps: 498, steps per second: 25, episode reward: 378.000, mean reward: 0.759 [-1.000, 1.000], mean action: 2.614 [0.000, 6.000], mean observation: 138.280 [0.000, 252.000], loss: 1.341630, mean_absolute_error: 28.145477, mean_q: 33.284569\n",
            "  75043/300000: episode: 174, duration: 11.929s, episode steps: 291, steps per second: 24, episode reward: 204.000, mean reward: 0.701 [-1.000, 1.000], mean action: 2.416 [0.000, 6.000], mean observation: 137.941 [0.000, 252.000], loss: 1.488122, mean_absolute_error: 28.158434, mean_q: 33.364677\n",
            "  75571/300000: episode: 175, duration: 21.538s, episode steps: 528, steps per second: 25, episode reward: 447.000, mean reward: 0.847 [-1.000, 1.000], mean action: 2.689 [0.000, 6.000], mean observation: 138.457 [0.000, 252.000], loss: 1.357390, mean_absolute_error: 28.145031, mean_q: 33.311981\n",
            "  76191/300000: episode: 176, duration: 25.485s, episode steps: 620, steps per second: 24, episode reward: 267.000, mean reward: 0.431 [-1.000, 1.000], mean action: 2.644 [0.000, 6.000], mean observation: 138.043 [0.000, 252.000], loss: 1.402440, mean_absolute_error: 28.252121, mean_q: 33.417061\n",
            "  76555/300000: episode: 177, duration: 15.055s, episode steps: 364, steps per second: 24, episode reward: 322.000, mean reward: 0.885 [-1.000, 1.000], mean action: 2.549 [0.000, 6.000], mean observation: 137.598 [0.000, 252.000], loss: 1.346845, mean_absolute_error: 28.219904, mean_q: 33.390572\n",
            "  76925/300000: episode: 178, duration: 15.080s, episode steps: 370, steps per second: 25, episode reward: 233.000, mean reward: 0.630 [-1.000, 1.000], mean action: 2.892 [0.000, 6.000], mean observation: 137.910 [0.000, 252.000], loss: 1.315093, mean_absolute_error: 28.227888, mean_q: 33.424488\n",
            "  77108/300000: episode: 179, duration: 7.525s, episode steps: 183, steps per second: 24, episode reward: 150.000, mean reward: 0.820 [-1.000, 1.000], mean action: 2.530 [0.000, 6.000], mean observation: 138.443 [0.000, 252.000], loss: 1.502255, mean_absolute_error: 28.206623, mean_q: 33.405510\n",
            "  77332/300000: episode: 180, duration: 9.181s, episode steps: 224, steps per second: 24, episode reward: 142.000, mean reward: 0.634 [-1.000, 1.000], mean action: 2.679 [0.000, 6.000], mean observation: 138.528 [0.000, 252.000], loss: 1.140369, mean_absolute_error: 28.147993, mean_q: 33.332924\n",
            "  77969/300000: episode: 181, duration: 35.329s, episode steps: 637, steps per second: 18, episode reward: 377.000, mean reward: 0.592 [-1.000, 1.000], mean action: 2.564 [0.000, 6.000], mean observation: 138.592 [0.000, 252.000], loss: 1.320559, mean_absolute_error: 28.258297, mean_q: 33.415031\n",
            "  78148/300000: episode: 182, duration: 11.582s, episode steps: 179, steps per second: 15, episode reward: 139.000, mean reward: 0.777 [-1.000, 1.000], mean action: 2.827 [0.000, 6.000], mean observation: 138.389 [0.000, 252.000], loss: 1.285357, mean_absolute_error: 28.201622, mean_q: 33.334984\n",
            "  78633/300000: episode: 183, duration: 20.121s, episode steps: 485, steps per second: 24, episode reward: 331.000, mean reward: 0.682 [-1.000, 1.000], mean action: 2.639 [0.000, 6.000], mean observation: 138.242 [0.000, 252.000], loss: 1.285012, mean_absolute_error: 28.276146, mean_q: 33.432171\n",
            "  78972/300000: episode: 184, duration: 13.950s, episode steps: 339, steps per second: 24, episode reward: 198.000, mean reward: 0.584 [-1.000, 1.000], mean action: 2.569 [0.000, 6.000], mean observation: 137.746 [0.000, 252.000], loss: 1.310301, mean_absolute_error: 28.250273, mean_q: 33.431335\n",
            "  79426/300000: episode: 185, duration: 18.530s, episode steps: 454, steps per second: 25, episode reward: 221.000, mean reward: 0.487 [-1.000, 1.000], mean action: 2.656 [0.000, 6.000], mean observation: 138.652 [0.000, 252.000], loss: 1.351224, mean_absolute_error: 28.178522, mean_q: 33.332565\n",
            "  79916/300000: episode: 186, duration: 19.948s, episode steps: 490, steps per second: 25, episode reward: 320.000, mean reward: 0.653 [-1.000, 1.000], mean action: 2.778 [0.000, 6.000], mean observation: 138.656 [0.000, 252.000], loss: 1.465798, mean_absolute_error: 28.129213, mean_q: 33.260151\n",
            "  80166/300000: episode: 187, duration: 10.338s, episode steps: 250, steps per second: 24, episode reward: 207.000, mean reward: 0.828 [-1.000, 1.000], mean action: 2.600 [0.000, 6.000], mean observation: 138.025 [0.000, 252.000], loss: 1.813423, mean_absolute_error: 28.361187, mean_q: 33.626968\n",
            "  80606/300000: episode: 188, duration: 17.909s, episode steps: 440, steps per second: 25, episode reward: 266.000, mean reward: 0.605 [-1.000, 1.000], mean action: 2.648 [0.000, 6.000], mean observation: 138.554 [0.000, 252.000], loss: 1.684754, mean_absolute_error: 28.407726, mean_q: 33.591679\n",
            "  80947/300000: episode: 189, duration: 14.023s, episode steps: 341, steps per second: 24, episode reward: 262.000, mean reward: 0.768 [-1.000, 1.000], mean action: 2.584 [0.000, 6.000], mean observation: 138.614 [0.000, 252.000], loss: 1.596040, mean_absolute_error: 28.399830, mean_q: 33.532242\n",
            "  81508/300000: episode: 190, duration: 22.918s, episode steps: 561, steps per second: 24, episode reward: 347.000, mean reward: 0.619 [-1.000, 1.000], mean action: 2.902 [0.000, 6.000], mean observation: 138.292 [0.000, 252.000], loss: 1.513801, mean_absolute_error: 28.427383, mean_q: 33.620476\n",
            "  81794/300000: episode: 191, duration: 19.488s, episode steps: 286, steps per second: 15, episode reward: 206.000, mean reward: 0.720 [-1.000, 1.000], mean action: 2.483 [0.000, 6.000], mean observation: 138.700 [0.000, 252.000], loss: 1.491189, mean_absolute_error: 28.426458, mean_q: 33.608139\n",
            "  82244/300000: episode: 192, duration: 22.340s, episode steps: 450, steps per second: 20, episode reward: 239.000, mean reward: 0.531 [-1.000, 1.000], mean action: 2.807 [0.000, 6.000], mean observation: 138.680 [0.000, 252.000], loss: 1.598399, mean_absolute_error: 28.463987, mean_q: 33.632446\n",
            "  82730/300000: episode: 193, duration: 19.867s, episode steps: 486, steps per second: 24, episode reward: 303.000, mean reward: 0.623 [-1.000, 1.000], mean action: 2.652 [0.000, 6.000], mean observation: 138.267 [0.000, 252.000], loss: 1.411417, mean_absolute_error: 28.461702, mean_q: 33.630222\n",
            "  83071/300000: episode: 194, duration: 13.913s, episode steps: 341, steps per second: 25, episode reward: 152.000, mean reward: 0.446 [-1.000, 1.000], mean action: 3.111 [0.000, 6.000], mean observation: 138.210 [0.000, 252.000], loss: 1.467408, mean_absolute_error: 28.396851, mean_q: 33.577145\n",
            "  83655/300000: episode: 195, duration: 23.870s, episode steps: 584, steps per second: 24, episode reward: 296.000, mean reward: 0.507 [-1.000, 1.000], mean action: 2.824 [0.000, 6.000], mean observation: 138.091 [0.000, 252.000], loss: 1.284253, mean_absolute_error: 28.417742, mean_q: 33.575836\n",
            "  83934/300000: episode: 196, duration: 11.389s, episode steps: 279, steps per second: 24, episode reward: 216.000, mean reward: 0.774 [-1.000, 1.000], mean action: 2.993 [0.000, 6.000], mean observation: 138.691 [0.000, 252.000], loss: 1.340528, mean_absolute_error: 28.412588, mean_q: 33.538391\n",
            "  84411/300000: episode: 197, duration: 19.463s, episode steps: 477, steps per second: 25, episode reward: 355.000, mean reward: 0.744 [-1.000, 1.000], mean action: 2.526 [0.000, 6.000], mean observation: 138.317 [0.000, 252.000], loss: 1.490706, mean_absolute_error: 28.330242, mean_q: 33.474312\n",
            "  84545/300000: episode: 198, duration: 5.497s, episode steps: 134, steps per second: 24, episode reward: 114.000, mean reward: 0.851 [-1.000, 1.000], mean action: 3.224 [0.000, 6.000], mean observation: 138.103 [0.000, 252.000], loss: 1.164090, mean_absolute_error: 28.315073, mean_q: 33.494366\n",
            "  84822/300000: episode: 199, duration: 11.339s, episode steps: 277, steps per second: 24, episode reward: 160.000, mean reward: 0.578 [-1.000, 1.000], mean action: 2.599 [0.000, 6.000], mean observation: 138.012 [0.000, 252.000], loss: 1.210204, mean_absolute_error: 28.367458, mean_q: 33.530563\n",
            "  85218/300000: episode: 200, duration: 16.312s, episode steps: 396, steps per second: 24, episode reward: 253.000, mean reward: 0.639 [-1.000, 1.000], mean action: 2.609 [0.000, 6.000], mean observation: 138.330 [0.000, 252.000], loss: 1.613290, mean_absolute_error: 28.390581, mean_q: 33.571613\n",
            "  85756/300000: episode: 201, duration: 32.831s, episode steps: 538, steps per second: 16, episode reward: 318.000, mean reward: 0.591 [-1.000, 1.000], mean action: 2.890 [0.000, 6.000], mean observation: 138.066 [0.000, 252.000], loss: 1.297843, mean_absolute_error: 28.450224, mean_q: 33.646034\n",
            "  86292/300000: episode: 202, duration: 26.084s, episode steps: 536, steps per second: 21, episode reward: 381.000, mean reward: 0.711 [-1.000, 1.000], mean action: 2.774 [0.000, 6.000], mean observation: 138.065 [0.000, 252.000], loss: 1.333515, mean_absolute_error: 28.399637, mean_q: 33.580986\n",
            "  86879/300000: episode: 203, duration: 23.884s, episode steps: 587, steps per second: 25, episode reward: 317.000, mean reward: 0.540 [-1.000, 1.000], mean action: 2.862 [0.000, 6.000], mean observation: 137.938 [0.000, 252.000], loss: 1.393137, mean_absolute_error: 28.372578, mean_q: 33.535839\n",
            "  87234/300000: episode: 204, duration: 14.544s, episode steps: 355, steps per second: 24, episode reward: 152.000, mean reward: 0.428 [-1.000, 1.000], mean action: 2.873 [0.000, 6.000], mean observation: 138.331 [0.000, 252.000], loss: 1.419012, mean_absolute_error: 28.394220, mean_q: 33.625362\n",
            "  87491/300000: episode: 205, duration: 10.536s, episode steps: 257, steps per second: 24, episode reward: 143.000, mean reward: 0.556 [-1.000, 1.000], mean action: 2.872 [0.000, 6.000], mean observation: 138.060 [0.000, 252.000], loss: 1.477165, mean_absolute_error: 28.333744, mean_q: 33.538017\n",
            "  87937/300000: episode: 206, duration: 18.048s, episode steps: 446, steps per second: 25, episode reward: 191.000, mean reward: 0.428 [-1.000, 1.000], mean action: 2.596 [0.000, 6.000], mean observation: 138.283 [0.000, 252.000], loss: 1.393528, mean_absolute_error: 28.370787, mean_q: 33.537876\n",
            "  88449/300000: episode: 207, duration: 20.882s, episode steps: 512, steps per second: 25, episode reward: 308.000, mean reward: 0.602 [-1.000, 1.000], mean action: 2.881 [0.000, 6.000], mean observation: 138.286 [0.000, 252.000], loss: 1.292617, mean_absolute_error: 28.391478, mean_q: 33.537102\n",
            "  88808/300000: episode: 208, duration: 14.711s, episode steps: 359, steps per second: 24, episode reward: 240.000, mean reward: 0.669 [-1.000, 1.000], mean action: 2.674 [0.000, 6.000], mean observation: 137.713 [0.000, 252.000], loss: 1.460159, mean_absolute_error: 28.345612, mean_q: 33.499786\n",
            "  89119/300000: episode: 209, duration: 12.898s, episode steps: 311, steps per second: 24, episode reward: 183.000, mean reward: 0.588 [-1.000, 1.000], mean action: 2.855 [0.000, 6.000], mean observation: 138.083 [0.000, 252.000], loss: 1.524224, mean_absolute_error: 28.321342, mean_q: 33.484238\n",
            "  89605/300000: episode: 210, duration: 19.928s, episode steps: 486, steps per second: 24, episode reward: 276.000, mean reward: 0.568 [-1.000, 1.000], mean action: 2.743 [0.000, 6.000], mean observation: 138.460 [0.000, 252.000], loss: 1.448597, mean_absolute_error: 28.341370, mean_q: 33.507721\n",
            "  89901/300000: episode: 211, duration: 27.927s, episode steps: 296, steps per second: 11, episode reward: 237.000, mean reward: 0.801 [-1.000, 1.000], mean action: 2.797 [0.000, 6.000], mean observation: 138.346 [0.000, 252.000], loss: 1.243419, mean_absolute_error: 28.269426, mean_q: 33.394737\n",
            "  90202/300000: episode: 212, duration: 16.532s, episode steps: 301, steps per second: 18, episode reward: 230.000, mean reward: 0.764 [-1.000, 1.000], mean action: 2.870 [0.000, 6.000], mean observation: 138.321 [0.000, 252.000], loss: 1.946544, mean_absolute_error: 28.248634, mean_q: 33.443058\n",
            "  90649/300000: episode: 213, duration: 18.362s, episode steps: 447, steps per second: 24, episode reward: 233.000, mean reward: 0.521 [-1.000, 1.000], mean action: 2.779 [0.000, 6.000], mean observation: 137.631 [0.000, 252.000], loss: 1.861000, mean_absolute_error: 28.225655, mean_q: 33.374218\n",
            "  91074/300000: episode: 214, duration: 17.342s, episode steps: 425, steps per second: 25, episode reward: 306.000, mean reward: 0.720 [-1.000, 1.000], mean action: 2.814 [0.000, 6.000], mean observation: 138.536 [0.000, 252.000], loss: 1.701636, mean_absolute_error: 28.113157, mean_q: 33.230865\n",
            "  91531/300000: episode: 215, duration: 18.677s, episode steps: 457, steps per second: 24, episode reward: 215.000, mean reward: 0.470 [-1.000, 1.000], mean action: 2.967 [0.000, 6.000], mean observation: 138.098 [0.000, 252.000], loss: 1.585558, mean_absolute_error: 28.173878, mean_q: 33.327938\n",
            "  91959/300000: episode: 216, duration: 17.399s, episode steps: 428, steps per second: 25, episode reward: 225.000, mean reward: 0.526 [-1.000, 1.000], mean action: 2.815 [0.000, 6.000], mean observation: 138.450 [0.000, 252.000], loss: 1.567212, mean_absolute_error: 28.179661, mean_q: 33.266830\n",
            "  92440/300000: episode: 217, duration: 19.688s, episode steps: 481, steps per second: 24, episode reward: 212.000, mean reward: 0.441 [-1.000, 1.000], mean action: 3.015 [0.000, 6.000], mean observation: 138.137 [0.000, 252.000], loss: 1.549790, mean_absolute_error: 28.202356, mean_q: 33.347061\n",
            "  92723/300000: episode: 218, duration: 11.709s, episode steps: 283, steps per second: 24, episode reward: 200.000, mean reward: 0.707 [-1.000, 1.000], mean action: 2.742 [0.000, 6.000], mean observation: 138.259 [0.000, 252.000], loss: 1.562046, mean_absolute_error: 28.163235, mean_q: 33.295227\n",
            "  92984/300000: episode: 219, duration: 10.780s, episode steps: 261, steps per second: 24, episode reward: 211.000, mean reward: 0.808 [-1.000, 1.000], mean action: 2.843 [0.000, 6.000], mean observation: 138.481 [0.000, 252.000], loss: 1.600199, mean_absolute_error: 28.214474, mean_q: 33.377094\n",
            "  93430/300000: episode: 220, duration: 18.155s, episode steps: 446, steps per second: 25, episode reward: 264.000, mean reward: 0.592 [-1.000, 1.000], mean action: 3.146 [0.000, 6.000], mean observation: 137.749 [0.000, 252.000], loss: 1.861437, mean_absolute_error: 28.147644, mean_q: 33.257549\n",
            "  93800/300000: episode: 221, duration: 23.621s, episode steps: 370, steps per second: 16, episode reward: 254.000, mean reward: 0.686 [-1.000, 1.000], mean action: 2.708 [0.000, 6.000], mean observation: 138.381 [0.000, 252.000], loss: 1.570098, mean_absolute_error: 28.231707, mean_q: 33.393646\n",
            "  94033/300000: episode: 222, duration: 14.061s, episode steps: 233, steps per second: 17, episode reward: 190.000, mean reward: 0.815 [-1.000, 1.000], mean action: 2.777 [0.000, 6.000], mean observation: 138.144 [0.000, 252.000], loss: 1.548160, mean_absolute_error: 28.177382, mean_q: 33.305580\n",
            "  94213/300000: episode: 223, duration: 7.451s, episode steps: 180, steps per second: 24, episode reward: 145.000, mean reward: 0.806 [-1.000, 1.000], mean action: 2.344 [0.000, 6.000], mean observation: 138.456 [0.000, 252.000], loss: 1.551545, mean_absolute_error: 28.218473, mean_q: 33.322742\n",
            "  94531/300000: episode: 224, duration: 13.033s, episode steps: 318, steps per second: 24, episode reward: 182.000, mean reward: 0.572 [-1.000, 1.000], mean action: 2.962 [0.000, 6.000], mean observation: 137.971 [0.000, 252.000], loss: 1.794718, mean_absolute_error: 28.167622, mean_q: 33.334763\n",
            "  94845/300000: episode: 225, duration: 12.829s, episode steps: 314, steps per second: 24, episode reward: 257.000, mean reward: 0.818 [-1.000, 1.000], mean action: 2.701 [0.000, 6.000], mean observation: 137.757 [0.000, 252.000], loss: 1.479180, mean_absolute_error: 28.172615, mean_q: 33.287373\n",
            "  95365/300000: episode: 226, duration: 21.326s, episode steps: 520, steps per second: 24, episode reward: 315.000, mean reward: 0.606 [-1.000, 1.000], mean action: 2.923 [0.000, 6.000], mean observation: 138.359 [0.000, 252.000], loss: 1.485586, mean_absolute_error: 28.213831, mean_q: 33.325600\n",
            "  95677/300000: episode: 227, duration: 12.565s, episode steps: 312, steps per second: 25, episode reward: 183.000, mean reward: 0.587 [-1.000, 1.000], mean action: 2.718 [0.000, 6.000], mean observation: 137.721 [0.000, 252.000], loss: 1.554114, mean_absolute_error: 28.152412, mean_q: 33.273708\n",
            "  95854/300000: episode: 228, duration: 7.185s, episode steps: 177, steps per second: 25, episode reward: 157.000, mean reward: 0.887 [-1.000, 1.000], mean action: 2.723 [0.000, 6.000], mean observation: 138.455 [0.000, 252.000], loss: 1.861402, mean_absolute_error: 28.149321, mean_q: 33.290752\n",
            "  96120/300000: episode: 229, duration: 10.702s, episode steps: 266, steps per second: 25, episode reward: 182.000, mean reward: 0.684 [-1.000, 1.000], mean action: 2.838 [0.000, 6.000], mean observation: 138.339 [0.000, 252.000], loss: 1.770717, mean_absolute_error: 28.215748, mean_q: 33.352936\n",
            "  96625/300000: episode: 230, duration: 20.239s, episode steps: 505, steps per second: 25, episode reward: 247.000, mean reward: 0.489 [-1.000, 1.000], mean action: 3.002 [0.000, 6.000], mean observation: 138.125 [0.000, 252.000], loss: 1.519627, mean_absolute_error: 28.164886, mean_q: 33.290348\n",
            "  97090/300000: episode: 231, duration: 28.582s, episode steps: 465, steps per second: 16, episode reward: 193.000, mean reward: 0.415 [-1.000, 1.000], mean action: 3.176 [0.000, 6.000], mean observation: 138.213 [0.000, 252.000], loss: 1.423982, mean_absolute_error: 28.147938, mean_q: 33.251537\n",
            "  97384/300000: episode: 232, duration: 16.748s, episode steps: 294, steps per second: 18, episode reward: 248.000, mean reward: 0.844 [-1.000, 1.000], mean action: 2.803 [0.000, 6.000], mean observation: 138.505 [0.000, 252.000], loss: 1.502599, mean_absolute_error: 28.188034, mean_q: 33.300846\n",
            "  97491/300000: episode: 233, duration: 4.409s, episode steps: 107, steps per second: 24, episode reward: 95.000, mean reward: 0.888 [-1.000, 1.000], mean action: 3.093 [0.000, 6.000], mean observation: 138.104 [0.000, 252.000], loss: 1.344924, mean_absolute_error: 28.156216, mean_q: 33.268780\n",
            "  97805/300000: episode: 234, duration: 12.590s, episode steps: 314, steps per second: 25, episode reward: 231.000, mean reward: 0.736 [-1.000, 1.000], mean action: 3.115 [0.000, 6.000], mean observation: 138.243 [0.000, 252.000], loss: 1.512016, mean_absolute_error: 28.063068, mean_q: 33.149529\n",
            "  98086/300000: episode: 235, duration: 11.339s, episode steps: 281, steps per second: 25, episode reward: 187.000, mean reward: 0.665 [-1.000, 1.000], mean action: 2.754 [0.000, 6.000], mean observation: 138.666 [0.000, 252.000], loss: 1.507372, mean_absolute_error: 28.168522, mean_q: 33.319965\n",
            "  98369/300000: episode: 236, duration: 11.459s, episode steps: 283, steps per second: 25, episode reward: 193.000, mean reward: 0.682 [-1.000, 1.000], mean action: 2.767 [0.000, 6.000], mean observation: 138.644 [0.000, 252.000], loss: 1.576716, mean_absolute_error: 28.104879, mean_q: 33.198143\n",
            "  98532/300000: episode: 237, duration: 6.645s, episode steps: 163, steps per second: 25, episode reward: 144.000, mean reward: 0.883 [-1.000, 1.000], mean action: 2.902 [0.000, 6.000], mean observation: 138.312 [0.000, 252.000], loss: 1.300146, mean_absolute_error: 28.347710, mean_q: 33.471767\n",
            "  98697/300000: episode: 238, duration: 6.729s, episode steps: 165, steps per second: 25, episode reward: 147.000, mean reward: 0.891 [-1.000, 1.000], mean action: 2.661 [0.000, 6.000], mean observation: 138.422 [0.000, 252.000], loss: 1.579415, mean_absolute_error: 28.241312, mean_q: 33.363129\n",
            "  99056/300000: episode: 239, duration: 14.517s, episode steps: 359, steps per second: 25, episode reward: 246.000, mean reward: 0.685 [-1.000, 1.000], mean action: 2.961 [0.000, 6.000], mean observation: 138.671 [0.000, 252.000], loss: 1.467935, mean_absolute_error: 28.180410, mean_q: 33.283615\n",
            "  99342/300000: episode: 240, duration: 11.641s, episode steps: 286, steps per second: 25, episode reward: 236.000, mean reward: 0.825 [-1.000, 1.000], mean action: 2.745 [0.000, 6.000], mean observation: 138.608 [0.000, 252.000], loss: 1.474595, mean_absolute_error: 28.234686, mean_q: 33.326950\n",
            "  99583/300000: episode: 241, duration: 19.163s, episode steps: 241, steps per second: 13, episode reward: 215.000, mean reward: 0.892 [-1.000, 1.000], mean action: 2.598 [0.000, 6.000], mean observation: 138.242 [0.000, 252.000], loss: 1.942670, mean_absolute_error: 28.241556, mean_q: 33.353592\n",
            "  99964/300000: episode: 242, duration: 19.793s, episode steps: 381, steps per second: 19, episode reward: 255.000, mean reward: 0.669 [-1.000, 1.000], mean action: 2.402 [0.000, 6.000], mean observation: 138.154 [0.000, 252.000], loss: 1.629168, mean_absolute_error: 28.163399, mean_q: 33.268803\n",
            " 100420/300000: episode: 243, duration: 18.420s, episode steps: 456, steps per second: 25, episode reward: 252.000, mean reward: 0.553 [-1.000, 1.000], mean action: 2.811 [0.000, 6.000], mean observation: 138.669 [0.000, 252.000], loss: 2.124310, mean_absolute_error: 28.481451, mean_q: 33.626144\n",
            " 100694/300000: episode: 244, duration: 11.122s, episode steps: 274, steps per second: 25, episode reward: 187.000, mean reward: 0.682 [-1.000, 1.000], mean action: 2.985 [0.000, 6.000], mean observation: 138.415 [0.000, 252.000], loss: 2.069286, mean_absolute_error: 28.522303, mean_q: 33.656754\n",
            " 101223/300000: episode: 245, duration: 21.367s, episode steps: 529, steps per second: 25, episode reward: 297.000, mean reward: 0.561 [-1.000, 1.000], mean action: 2.802 [0.000, 6.000], mean observation: 138.444 [0.000, 252.000], loss: 1.937737, mean_absolute_error: 28.524630, mean_q: 33.694054\n",
            " 101615/300000: episode: 246, duration: 15.879s, episode steps: 392, steps per second: 25, episode reward: 202.000, mean reward: 0.515 [-1.000, 1.000], mean action: 2.694 [0.000, 6.000], mean observation: 137.678 [0.000, 252.000], loss: 1.625570, mean_absolute_error: 28.430965, mean_q: 33.567112\n",
            " 101912/300000: episode: 247, duration: 11.963s, episode steps: 297, steps per second: 25, episode reward: 263.000, mean reward: 0.886 [-1.000, 1.000], mean action: 2.805 [0.000, 6.000], mean observation: 138.737 [0.000, 252.000], loss: 1.928506, mean_absolute_error: 28.440781, mean_q: 33.577957\n",
            " 102156/300000: episode: 248, duration: 9.945s, episode steps: 244, steps per second: 25, episode reward: 188.000, mean reward: 0.770 [-1.000, 1.000], mean action: 2.803 [0.000, 6.000], mean observation: 138.689 [0.000, 252.000], loss: 1.784434, mean_absolute_error: 28.360159, mean_q: 33.495083\n",
            " 102428/300000: episode: 249, duration: 11.038s, episode steps: 272, steps per second: 25, episode reward: 183.000, mean reward: 0.673 [-1.000, 1.000], mean action: 2.213 [0.000, 6.000], mean observation: 138.033 [0.000, 252.000], loss: 1.632734, mean_absolute_error: 28.340368, mean_q: 33.471294\n",
            " 102945/300000: episode: 250, duration: 21.152s, episode steps: 517, steps per second: 24, episode reward: 404.000, mean reward: 0.781 [-1.000, 1.000], mean action: 2.625 [0.000, 6.000], mean observation: 138.468 [0.000, 252.000], loss: 1.823006, mean_absolute_error: 28.417980, mean_q: 33.583801\n",
            " 103322/300000: episode: 251, duration: 24.580s, episode steps: 377, steps per second: 15, episode reward: 185.000, mean reward: 0.491 [-1.000, 1.000], mean action: 2.568 [0.000, 6.000], mean observation: 137.915 [0.000, 252.000], loss: 1.865618, mean_absolute_error: 28.335619, mean_q: 33.501205\n",
            " 103558/300000: episode: 252, duration: 14.567s, episode steps: 236, steps per second: 16, episode reward: 195.000, mean reward: 0.826 [-1.000, 1.000], mean action: 2.699 [0.000, 6.000], mean observation: 137.947 [0.000, 252.000], loss: 1.857674, mean_absolute_error: 28.477308, mean_q: 33.687935\n",
            " 104691/300000: episode: 253, duration: 48.762s, episode steps: 1133, steps per second: 23, episode reward: 579.000, mean reward: 0.511 [-1.000, 1.000], mean action: 3.029 [0.000, 6.000], mean observation: 94.606 [0.000, 252.000], loss: 1.575140, mean_absolute_error: 28.471786, mean_q: 33.634331\n",
            " 105022/300000: episode: 254, duration: 13.455s, episode steps: 331, steps per second: 25, episode reward: 229.000, mean reward: 0.692 [-1.000, 1.000], mean action: 3.051 [0.000, 6.000], mean observation: 138.325 [0.000, 252.000], loss: 1.738709, mean_absolute_error: 28.538183, mean_q: 33.729656\n",
            " 105499/300000: episode: 255, duration: 19.434s, episode steps: 477, steps per second: 25, episode reward: 322.000, mean reward: 0.675 [-1.000, 1.000], mean action: 2.927 [0.000, 6.000], mean observation: 138.290 [0.000, 252.000], loss: 1.583071, mean_absolute_error: 28.597887, mean_q: 33.783634\n",
            " 105938/300000: episode: 256, duration: 17.815s, episode steps: 439, steps per second: 25, episode reward: 335.000, mean reward: 0.763 [-1.000, 1.000], mean action: 2.740 [0.000, 6.000], mean observation: 138.292 [0.000, 252.000], loss: 1.603139, mean_absolute_error: 28.645727, mean_q: 33.819172\n",
            " 106389/300000: episode: 257, duration: 18.704s, episode steps: 451, steps per second: 24, episode reward: 223.000, mean reward: 0.494 [-1.000, 1.000], mean action: 2.734 [0.000, 6.000], mean observation: 138.131 [0.000, 252.000], loss: 1.806597, mean_absolute_error: 28.485111, mean_q: 33.656048\n",
            " 106758/300000: episode: 258, duration: 14.993s, episode steps: 369, steps per second: 25, episode reward: 209.000, mean reward: 0.566 [-1.000, 1.000], mean action: 3.003 [0.000, 6.000], mean observation: 138.311 [0.000, 252.000], loss: 1.489932, mean_absolute_error: 28.617121, mean_q: 33.845085\n",
            " 107221/300000: episode: 259, duration: 18.556s, episode steps: 463, steps per second: 25, episode reward: 303.000, mean reward: 0.654 [-1.000, 1.000], mean action: 2.482 [0.000, 6.000], mean observation: 138.124 [0.000, 252.000], loss: 1.441363, mean_absolute_error: 28.556286, mean_q: 33.723701\n",
            " 107554/300000: episode: 260, duration: 13.406s, episode steps: 333, steps per second: 25, episode reward: 226.000, mean reward: 0.679 [-1.000, 1.000], mean action: 2.523 [0.000, 6.000], mean observation: 138.015 [0.000, 252.000], loss: 1.499049, mean_absolute_error: 28.634695, mean_q: 33.843990\n",
            " 107861/300000: episode: 261, duration: 21.156s, episode steps: 307, steps per second: 15, episode reward: 226.000, mean reward: 0.736 [-1.000, 1.000], mean action: 2.417 [0.000, 6.000], mean observation: 138.016 [0.000, 252.000], loss: 1.694770, mean_absolute_error: 28.456589, mean_q: 33.697323\n",
            " 108392/300000: episode: 262, duration: 28.885s, episode steps: 531, steps per second: 18, episode reward: 224.000, mean reward: 0.422 [-1.000, 1.000], mean action: 2.454 [0.000, 6.000], mean observation: 137.948 [0.000, 252.000], loss: 1.396590, mean_absolute_error: 28.511278, mean_q: 33.691399\n",
            " 108805/300000: episode: 263, duration: 16.821s, episode steps: 413, steps per second: 25, episode reward: 192.000, mean reward: 0.465 [-1.000, 1.000], mean action: 2.731 [0.000, 6.000], mean observation: 137.901 [0.000, 252.000], loss: 1.468937, mean_absolute_error: 28.575846, mean_q: 33.752781\n",
            " 109102/300000: episode: 264, duration: 12.104s, episode steps: 297, steps per second: 25, episode reward: 195.000, mean reward: 0.657 [-1.000, 1.000], mean action: 2.697 [0.000, 6.000], mean observation: 138.583 [0.000, 252.000], loss: 1.873319, mean_absolute_error: 28.522957, mean_q: 33.722244\n",
            " 109492/300000: episode: 265, duration: 15.936s, episode steps: 390, steps per second: 24, episode reward: 221.000, mean reward: 0.567 [-1.000, 1.000], mean action: 2.782 [0.000, 6.000], mean observation: 138.179 [0.000, 252.000], loss: 1.622517, mean_absolute_error: 28.566784, mean_q: 33.771290\n",
            " 109959/300000: episode: 266, duration: 18.902s, episode steps: 467, steps per second: 25, episode reward: 206.000, mean reward: 0.441 [-1.000, 1.000], mean action: 2.617 [0.000, 6.000], mean observation: 137.408 [0.000, 252.000], loss: 1.590414, mean_absolute_error: 28.510885, mean_q: 33.725811\n",
            " 110282/300000: episode: 267, duration: 13.004s, episode steps: 323, steps per second: 25, episode reward: 249.000, mean reward: 0.771 [-1.000, 1.000], mean action: 2.793 [0.000, 6.000], mean observation: 138.286 [0.000, 252.000], loss: 2.172080, mean_absolute_error: 29.015928, mean_q: 34.375111\n",
            " 110747/300000: episode: 268, duration: 18.698s, episode steps: 465, steps per second: 25, episode reward: 285.000, mean reward: 0.613 [-1.000, 1.000], mean action: 2.946 [0.000, 6.000], mean observation: 138.306 [0.000, 252.000], loss: 2.274955, mean_absolute_error: 29.068817, mean_q: 34.369946\n",
            " 111181/300000: episode: 269, duration: 17.643s, episode steps: 434, steps per second: 25, episode reward: 212.000, mean reward: 0.488 [-1.000, 1.000], mean action: 2.929 [0.000, 6.000], mean observation: 138.095 [0.000, 252.000], loss: 2.170024, mean_absolute_error: 29.031780, mean_q: 34.345654\n",
            " 111560/300000: episode: 270, duration: 15.408s, episode steps: 379, steps per second: 25, episode reward: 293.000, mean reward: 0.773 [-1.000, 1.000], mean action: 2.741 [0.000, 6.000], mean observation: 138.341 [0.000, 252.000], loss: 2.035948, mean_absolute_error: 28.949263, mean_q: 34.235172\n",
            " 111881/300000: episode: 271, duration: 22.845s, episode steps: 321, steps per second: 14, episode reward: 214.000, mean reward: 0.667 [-1.000, 1.000], mean action: 2.829 [0.000, 6.000], mean observation: 138.230 [0.000, 252.000], loss: 1.726770, mean_absolute_error: 28.947533, mean_q: 34.236698\n",
            " 112130/300000: episode: 272, duration: 15.106s, episode steps: 249, steps per second: 16, episode reward: 192.000, mean reward: 0.771 [-1.000, 1.000], mean action: 2.859 [0.000, 6.000], mean observation: 138.730 [0.000, 252.000], loss: 1.873023, mean_absolute_error: 29.013855, mean_q: 34.374397\n",
            " 112250/300000: episode: 273, duration: 4.982s, episode steps: 120, steps per second: 24, episode reward: 104.000, mean reward: 0.867 [-1.000, 1.000], mean action: 2.867 [0.000, 6.000], mean observation: 137.989 [0.000, 252.000], loss: 1.985570, mean_absolute_error: 29.053118, mean_q: 34.373707\n",
            " 112685/300000: episode: 274, duration: 17.648s, episode steps: 435, steps per second: 25, episode reward: 327.000, mean reward: 0.752 [-1.000, 1.000], mean action: 2.579 [0.000, 6.000], mean observation: 138.415 [0.000, 252.000], loss: 1.934413, mean_absolute_error: 29.033279, mean_q: 34.319733\n",
            " 112917/300000: episode: 275, duration: 9.485s, episode steps: 232, steps per second: 24, episode reward: 143.000, mean reward: 0.616 [-1.000, 1.000], mean action: 2.478 [0.000, 6.000], mean observation: 138.640 [0.000, 252.000], loss: 1.832173, mean_absolute_error: 28.967068, mean_q: 34.293495\n",
            " 113281/300000: episode: 276, duration: 15.365s, episode steps: 364, steps per second: 24, episode reward: 226.000, mean reward: 0.621 [-1.000, 1.000], mean action: 2.997 [0.000, 6.000], mean observation: 138.198 [0.000, 252.000], loss: 2.028424, mean_absolute_error: 28.963068, mean_q: 34.281326\n",
            " 113695/300000: episode: 277, duration: 16.859s, episode steps: 414, steps per second: 25, episode reward: 245.000, mean reward: 0.592 [-1.000, 1.000], mean action: 2.710 [0.000, 6.000], mean observation: 138.609 [0.000, 252.000], loss: 1.829347, mean_absolute_error: 29.080235, mean_q: 34.398083\n",
            " 113830/300000: episode: 278, duration: 5.594s, episode steps: 135, steps per second: 24, episode reward: 115.000, mean reward: 0.852 [-1.000, 1.000], mean action: 2.800 [0.000, 6.000], mean observation: 138.024 [0.000, 252.000], loss: 1.903840, mean_absolute_error: 29.038376, mean_q: 34.320755\n",
            " 113959/300000: episode: 279, duration: 5.375s, episode steps: 129, steps per second: 24, episode reward: 97.000, mean reward: 0.752 [-1.000, 1.000], mean action: 3.093 [0.000, 6.000], mean observation: 138.051 [0.000, 252.000], loss: 2.182812, mean_absolute_error: 29.021439, mean_q: 34.360405\n",
            " 114298/300000: episode: 280, duration: 13.811s, episode steps: 339, steps per second: 25, episode reward: 255.000, mean reward: 0.752 [-1.000, 1.000], mean action: 2.782 [0.000, 6.000], mean observation: 138.527 [0.000, 252.000], loss: 1.838734, mean_absolute_error: 28.999491, mean_q: 34.323826\n",
            " 114573/300000: episode: 281, duration: 20.665s, episode steps: 275, steps per second: 13, episode reward: 189.000, mean reward: 0.687 [-1.000, 1.000], mean action: 2.975 [0.000, 6.000], mean observation: 138.223 [0.000, 252.000], loss: 1.757126, mean_absolute_error: 29.074528, mean_q: 34.433826\n",
            " 114873/300000: episode: 282, duration: 16.636s, episode steps: 300, steps per second: 18, episode reward: 235.000, mean reward: 0.783 [-1.000, 1.000], mean action: 2.733 [0.000, 6.000], mean observation: 138.548 [0.000, 252.000], loss: 1.885432, mean_absolute_error: 28.968281, mean_q: 34.333508\n",
            " 115233/300000: episode: 283, duration: 14.618s, episode steps: 360, steps per second: 25, episode reward: 235.000, mean reward: 0.653 [-1.000, 1.000], mean action: 2.819 [0.000, 6.000], mean observation: 138.869 [0.000, 252.000], loss: 1.846817, mean_absolute_error: 28.956808, mean_q: 34.332008\n",
            " 115756/300000: episode: 284, duration: 21.135s, episode steps: 523, steps per second: 25, episode reward: 367.000, mean reward: 0.702 [-1.000, 1.000], mean action: 2.706 [0.000, 6.000], mean observation: 138.443 [0.000, 252.000], loss: 1.709436, mean_absolute_error: 29.031546, mean_q: 34.395466\n",
            " 116046/300000: episode: 285, duration: 11.774s, episode steps: 290, steps per second: 25, episode reward: 211.000, mean reward: 0.728 [-1.000, 1.000], mean action: 2.648 [0.000, 6.000], mean observation: 138.235 [0.000, 252.000], loss: 2.062096, mean_absolute_error: 29.098650, mean_q: 34.440292\n",
            " 116293/300000: episode: 286, duration: 10.057s, episode steps: 247, steps per second: 25, episode reward: 230.000, mean reward: 0.931 [-1.000, 1.000], mean action: 2.607 [0.000, 6.000], mean observation: 137.579 [0.000, 252.000], loss: 1.648592, mean_absolute_error: 29.050318, mean_q: 34.365540\n",
            " 116700/300000: episode: 287, duration: 16.396s, episode steps: 407, steps per second: 25, episode reward: 201.000, mean reward: 0.494 [-1.000, 1.000], mean action: 3.039 [0.000, 6.000], mean observation: 137.830 [0.000, 252.000], loss: 1.839146, mean_absolute_error: 28.979259, mean_q: 34.294613\n",
            " 116926/300000: episode: 288, duration: 9.120s, episode steps: 226, steps per second: 25, episode reward: 162.000, mean reward: 0.717 [-1.000, 1.000], mean action: 2.863 [0.000, 6.000], mean observation: 137.914 [0.000, 252.000], loss: 1.738583, mean_absolute_error: 28.950266, mean_q: 34.281914\n",
            " 117276/300000: episode: 289, duration: 14.146s, episode steps: 350, steps per second: 25, episode reward: 284.000, mean reward: 0.811 [-1.000, 1.000], mean action: 2.611 [0.000, 6.000], mean observation: 137.748 [0.000, 252.000], loss: 2.058262, mean_absolute_error: 28.959333, mean_q: 34.305637\n",
            " 117791/300000: episode: 290, duration: 20.695s, episode steps: 515, steps per second: 25, episode reward: 257.000, mean reward: 0.499 [-1.000, 1.000], mean action: 2.744 [0.000, 6.000], mean observation: 138.099 [0.000, 252.000], loss: 1.770990, mean_absolute_error: 29.005472, mean_q: 34.323776\n",
            " 118029/300000: episode: 291, duration: 18.151s, episode steps: 238, steps per second: 13, episode reward: 195.000, mean reward: 0.819 [-1.000, 1.000], mean action: 2.265 [0.000, 6.000], mean observation: 138.423 [0.000, 252.000], loss: 1.775308, mean_absolute_error: 29.235302, mean_q: 34.635708\n",
            " 118463/300000: episode: 292, duration: 22.696s, episode steps: 434, steps per second: 19, episode reward: 375.000, mean reward: 0.864 [-1.000, 1.000], mean action: 2.624 [0.000, 6.000], mean observation: 137.861 [0.000, 252.000], loss: 1.819505, mean_absolute_error: 29.044392, mean_q: 34.405590\n",
            " 118850/300000: episode: 293, duration: 15.461s, episode steps: 387, steps per second: 25, episode reward: 250.000, mean reward: 0.646 [-1.000, 1.000], mean action: 2.571 [0.000, 6.000], mean observation: 138.616 [0.000, 252.000], loss: 1.909247, mean_absolute_error: 29.025517, mean_q: 34.411098\n",
            " 119111/300000: episode: 294, duration: 10.568s, episode steps: 261, steps per second: 25, episode reward: 210.000, mean reward: 0.805 [-1.000, 1.000], mean action: 2.770 [0.000, 6.000], mean observation: 138.697 [0.000, 252.000], loss: 2.056652, mean_absolute_error: 29.005205, mean_q: 34.366077\n",
            " 119276/300000: episode: 295, duration: 6.741s, episode steps: 165, steps per second: 24, episode reward: 142.000, mean reward: 0.861 [-1.000, 1.000], mean action: 2.867 [0.000, 6.000], mean observation: 138.449 [0.000, 252.000], loss: 1.630357, mean_absolute_error: 29.158327, mean_q: 34.531853\n",
            " 119669/300000: episode: 296, duration: 15.937s, episode steps: 393, steps per second: 25, episode reward: 272.000, mean reward: 0.692 [-1.000, 1.000], mean action: 2.730 [0.000, 6.000], mean observation: 138.340 [0.000, 252.000], loss: 1.920646, mean_absolute_error: 29.015347, mean_q: 34.390743\n",
            " 119956/300000: episode: 297, duration: 11.722s, episode steps: 287, steps per second: 24, episode reward: 210.000, mean reward: 0.732 [-1.000, 1.000], mean action: 2.686 [0.000, 6.000], mean observation: 138.475 [0.000, 252.000], loss: 1.667819, mean_absolute_error: 29.188044, mean_q: 34.622169\n",
            " 120208/300000: episode: 298, duration: 10.486s, episode steps: 252, steps per second: 24, episode reward: 193.000, mean reward: 0.766 [-1.000, 1.000], mean action: 2.944 [0.000, 6.000], mean observation: 138.268 [0.000, 252.000], loss: 2.152548, mean_absolute_error: 29.287941, mean_q: 34.669823\n",
            " 120399/300000: episode: 299, duration: 7.860s, episode steps: 191, steps per second: 24, episode reward: 152.000, mean reward: 0.796 [-1.000, 1.000], mean action: 3.262 [0.000, 6.000], mean observation: 138.267 [0.000, 252.000], loss: 2.493542, mean_absolute_error: 29.356478, mean_q: 34.733383\n",
            " 120829/300000: episode: 300, duration: 17.491s, episode steps: 430, steps per second: 25, episode reward: 286.000, mean reward: 0.665 [-1.000, 1.000], mean action: 2.860 [0.000, 6.000], mean observation: 138.475 [0.000, 252.000], loss: 2.178431, mean_absolute_error: 29.382063, mean_q: 34.789963\n",
            " 121049/300000: episode: 301, duration: 20.304s, episode steps: 220, steps per second: 11, episode reward: 165.000, mean reward: 0.750 [-1.000, 1.000], mean action: 2.859 [0.000, 6.000], mean observation: 138.684 [0.000, 252.000], loss: 2.077171, mean_absolute_error: 29.365456, mean_q: 34.794739\n",
            " 121391/300000: episode: 302, duration: 18.708s, episode steps: 342, steps per second: 18, episode reward: 242.000, mean reward: 0.708 [-1.000, 1.000], mean action: 2.711 [0.000, 6.000], mean observation: 138.223 [0.000, 252.000], loss: 2.076849, mean_absolute_error: 29.320168, mean_q: 34.759785\n",
            " 121812/300000: episode: 303, duration: 17.097s, episode steps: 421, steps per second: 25, episode reward: 257.000, mean reward: 0.610 [-1.000, 1.000], mean action: 2.496 [0.000, 6.000], mean observation: 138.577 [0.000, 252.000], loss: 1.793205, mean_absolute_error: 29.386335, mean_q: 34.814434\n",
            " 121924/300000: episode: 304, duration: 4.685s, episode steps: 112, steps per second: 24, episode reward: 99.000, mean reward: 0.884 [-1.000, 1.000], mean action: 2.188 [0.000, 6.000], mean observation: 138.027 [0.000, 252.000], loss: 2.704612, mean_absolute_error: 29.331497, mean_q: 34.755253\n",
            " 122039/300000: episode: 305, duration: 4.765s, episode steps: 115, steps per second: 24, episode reward: 97.000, mean reward: 0.843 [-1.000, 1.000], mean action: 2.609 [0.000, 6.000], mean observation: 138.082 [0.000, 252.000], loss: 1.586789, mean_absolute_error: 29.508978, mean_q: 34.890537\n",
            " 122238/300000: episode: 306, duration: 8.169s, episode steps: 199, steps per second: 24, episode reward: 147.000, mean reward: 0.739 [-1.000, 1.000], mean action: 2.744 [0.000, 6.000], mean observation: 138.665 [0.000, 252.000], loss: 1.845725, mean_absolute_error: 29.412861, mean_q: 34.812901\n",
            " 122491/300000: episode: 307, duration: 10.360s, episode steps: 253, steps per second: 24, episode reward: 145.000, mean reward: 0.573 [-1.000, 1.000], mean action: 2.676 [0.000, 6.000], mean observation: 138.166 [0.000, 252.000], loss: 2.158080, mean_absolute_error: 29.349028, mean_q: 34.722801\n",
            " 122828/300000: episode: 308, duration: 13.661s, episode steps: 337, steps per second: 25, episode reward: 295.000, mean reward: 0.875 [-1.000, 1.000], mean action: 2.973 [0.000, 6.000], mean observation: 138.346 [0.000, 252.000], loss: 2.251053, mean_absolute_error: 29.430693, mean_q: 34.841125\n",
            " 123283/300000: episode: 309, duration: 18.456s, episode steps: 455, steps per second: 25, episode reward: 258.000, mean reward: 0.567 [-1.000, 1.000], mean action: 2.719 [0.000, 6.000], mean observation: 138.253 [0.000, 252.000], loss: 1.775573, mean_absolute_error: 29.384579, mean_q: 34.808479\n",
            " 123485/300000: episode: 310, duration: 8.246s, episode steps: 202, steps per second: 24, episode reward: 146.000, mean reward: 0.723 [-1.000, 1.000], mean action: 3.173 [0.000, 6.000], mean observation: 138.490 [0.000, 252.000], loss: 1.629028, mean_absolute_error: 29.468891, mean_q: 34.919147\n",
            " 123832/300000: episode: 311, duration: 25.153s, episode steps: 347, steps per second: 14, episode reward: 263.000, mean reward: 0.758 [-1.000, 1.000], mean action: 3.061 [0.000, 6.000], mean observation: 138.047 [0.000, 252.000], loss: 1.759507, mean_absolute_error: 29.405794, mean_q: 34.830940\n",
            " 124280/300000: episode: 312, duration: 22.904s, episode steps: 448, steps per second: 20, episode reward: 315.000, mean reward: 0.703 [-1.000, 1.000], mean action: 2.650 [0.000, 6.000], mean observation: 138.600 [0.000, 252.000], loss: 1.711115, mean_absolute_error: 29.421728, mean_q: 34.803001\n",
            " 124634/300000: episode: 313, duration: 14.487s, episode steps: 354, steps per second: 24, episode reward: 161.000, mean reward: 0.455 [-1.000, 1.000], mean action: 3.345 [0.000, 6.000], mean observation: 138.215 [0.000, 252.000], loss: 2.080011, mean_absolute_error: 29.350279, mean_q: 34.770092\n",
            " 124953/300000: episode: 314, duration: 13.060s, episode steps: 319, steps per second: 24, episode reward: 252.000, mean reward: 0.790 [-1.000, 1.000], mean action: 3.094 [0.000, 6.000], mean observation: 138.510 [0.000, 252.000], loss: 1.889812, mean_absolute_error: 29.515984, mean_q: 34.946342\n",
            " 125450/300000: episode: 315, duration: 20.106s, episode steps: 497, steps per second: 25, episode reward: 239.000, mean reward: 0.481 [-1.000, 1.000], mean action: 2.680 [0.000, 6.000], mean observation: 138.442 [0.000, 252.000], loss: 1.626278, mean_absolute_error: 29.494843, mean_q: 34.925346\n",
            " 126045/300000: episode: 316, duration: 24.122s, episode steps: 595, steps per second: 25, episode reward: 331.000, mean reward: 0.556 [-1.000, 1.000], mean action: 2.837 [0.000, 6.000], mean observation: 138.457 [0.000, 252.000], loss: 1.690418, mean_absolute_error: 29.429504, mean_q: 34.836807\n",
            " 126552/300000: episode: 317, duration: 20.508s, episode steps: 507, steps per second: 25, episode reward: 244.000, mean reward: 0.481 [-1.000, 1.000], mean action: 2.874 [0.000, 6.000], mean observation: 138.190 [0.000, 252.000], loss: 1.504015, mean_absolute_error: 29.404985, mean_q: 34.840645\n",
            " 126978/300000: episode: 318, duration: 17.476s, episode steps: 426, steps per second: 24, episode reward: 266.000, mean reward: 0.624 [-1.000, 1.000], mean action: 2.845 [0.000, 6.000], mean observation: 138.474 [0.000, 252.000], loss: 1.951533, mean_absolute_error: 29.370846, mean_q: 34.788780\n",
            " 127231/300000: episode: 319, duration: 10.434s, episode steps: 253, steps per second: 24, episode reward: 208.000, mean reward: 0.822 [-1.000, 1.000], mean action: 2.589 [0.000, 6.000], mean observation: 138.703 [0.000, 252.000], loss: 1.628152, mean_absolute_error: 29.420824, mean_q: 34.822136\n",
            " 127759/300000: episode: 320, duration: 21.374s, episode steps: 528, steps per second: 25, episode reward: 348.000, mean reward: 0.659 [-1.000, 1.000], mean action: 2.871 [0.000, 6.000], mean observation: 137.874 [0.000, 252.000], loss: 1.639941, mean_absolute_error: 29.355885, mean_q: 34.757835\n",
            " 127925/300000: episode: 321, duration: 15.750s, episode steps: 166, steps per second: 11, episode reward: 137.000, mean reward: 0.825 [-1.000, 1.000], mean action: 2.657 [0.000, 6.000], mean observation: 138.568 [0.000, 252.000], loss: 1.651468, mean_absolute_error: 29.381504, mean_q: 34.753876\n",
            " 128135/300000: episode: 322, duration: 13.469s, episode steps: 210, steps per second: 16, episode reward: 171.000, mean reward: 0.814 [-1.000, 1.000], mean action: 2.838 [0.000, 6.000], mean observation: 138.371 [0.000, 252.000], loss: 1.691104, mean_absolute_error: 29.337324, mean_q: 34.720390\n",
            " 128542/300000: episode: 323, duration: 16.502s, episode steps: 407, steps per second: 25, episode reward: 256.000, mean reward: 0.629 [-1.000, 1.000], mean action: 3.066 [0.000, 6.000], mean observation: 138.477 [0.000, 252.000], loss: 1.912513, mean_absolute_error: 29.435085, mean_q: 34.829170\n",
            " 129002/300000: episode: 324, duration: 18.659s, episode steps: 460, steps per second: 25, episode reward: 321.000, mean reward: 0.698 [-1.000, 1.000], mean action: 2.907 [0.000, 6.000], mean observation: 138.354 [0.000, 252.000], loss: 1.613489, mean_absolute_error: 29.259928, mean_q: 34.631290\n",
            " 129430/300000: episode: 325, duration: 17.408s, episode steps: 428, steps per second: 25, episode reward: 261.000, mean reward: 0.610 [-1.000, 1.000], mean action: 2.675 [0.000, 6.000], mean observation: 138.554 [0.000, 252.000], loss: 1.957964, mean_absolute_error: 29.316658, mean_q: 34.684395\n",
            " 130143/300000: episode: 326, duration: 28.930s, episode steps: 713, steps per second: 25, episode reward: 354.000, mean reward: 0.496 [-1.000, 1.000], mean action: 2.965 [0.000, 6.000], mean observation: 138.469 [0.000, 252.000], loss: 1.825460, mean_absolute_error: 29.378227, mean_q: 34.761238\n",
            " 130431/300000: episode: 327, duration: 11.719s, episode steps: 288, steps per second: 25, episode reward: 225.000, mean reward: 0.781 [-1.000, 1.000], mean action: 2.799 [0.000, 6.000], mean observation: 138.464 [0.000, 252.000], loss: 2.302042, mean_absolute_error: 29.520821, mean_q: 34.927155\n",
            " 130766/300000: episode: 328, duration: 13.412s, episode steps: 335, steps per second: 25, episode reward: 225.000, mean reward: 0.672 [-1.000, 1.000], mean action: 2.746 [0.000, 6.000], mean observation: 138.390 [0.000, 252.000], loss: 2.157509, mean_absolute_error: 29.514954, mean_q: 34.964390\n",
            " 130990/300000: episode: 329, duration: 9.016s, episode steps: 224, steps per second: 25, episode reward: 200.000, mean reward: 0.893 [-1.000, 1.000], mean action: 2.821 [0.000, 6.000], mean observation: 138.530 [0.000, 252.000], loss: 2.209705, mean_absolute_error: 29.650381, mean_q: 35.066135\n",
            " 131790/300000: episode: 330, duration: 32.017s, episode steps: 800, steps per second: 25, episode reward: 424.000, mean reward: 0.530 [-1.000, 1.000], mean action: 3.004 [0.000, 6.000], mean observation: 137.663 [0.000, 252.000], loss: 2.168236, mean_absolute_error: 29.691763, mean_q: 35.157990\n",
            " 132266/300000: episode: 331, duration: 34.954s, episode steps: 476, steps per second: 14, episode reward: 299.000, mean reward: 0.628 [-1.000, 1.000], mean action: 2.815 [0.000, 6.000], mean observation: 137.813 [0.000, 252.000], loss: 1.910879, mean_absolute_error: 29.672043, mean_q: 35.104721\n",
            " 132534/300000: episode: 332, duration: 15.329s, episode steps: 268, steps per second: 17, episode reward: 195.000, mean reward: 0.728 [-1.000, 1.000], mean action: 2.701 [0.000, 6.000], mean observation: 138.010 [0.000, 252.000], loss: 1.621199, mean_absolute_error: 29.836271, mean_q: 35.292160\n",
            " 132994/300000: episode: 333, duration: 18.731s, episode steps: 460, steps per second: 25, episode reward: 273.000, mean reward: 0.593 [-1.000, 1.000], mean action: 2.498 [0.000, 6.000], mean observation: 137.823 [0.000, 252.000], loss: 1.789703, mean_absolute_error: 29.890825, mean_q: 35.357143\n",
            " 133385/300000: episode: 334, duration: 16.176s, episode steps: 391, steps per second: 24, episode reward: 291.000, mean reward: 0.744 [-1.000, 1.000], mean action: 2.371 [0.000, 6.000], mean observation: 137.885 [0.000, 252.000], loss: 2.073097, mean_absolute_error: 29.899826, mean_q: 35.380138\n",
            "done, took 5898.112 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rqP6XEBzmbEw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### カラーで実行\n",
        "遅すぎ"
      ]
    },
    {
      "metadata": {
        "id": "dbj1SheM3-PB",
        "colab_type": "code",
        "outputId": "4dd808ba-6370-458b-d983-bf4fa898adc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1963
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import keras\n",
        "#import pydot\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Conv2D, Reshape, MaxPooling2D, Permute\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import plot_model\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy, MaxBoltzmannQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
        "\n",
        "from nes_py.wrappers import BinarySpaceToDiscreteSpaceEnv\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
        "\n",
        "from gym import wrappers\n",
        "\n",
        "from keras import backend as K # 柴田\n",
        "\n",
        "\n",
        "ENV_NAME = 'SuperMarioBros-v0'\n",
        "\n",
        "# Get the environment and extract the number of actions.\n",
        "env = gym_super_mario_bros.make(ENV_NAME)\n",
        "env = BinarySpaceToDiscreteSpaceEnv(env, SIMPLE_MOVEMENT)\n",
        "\n",
        "env = wrappers.Monitor(env, './drive/ProX/mario3', force=False, video_callable=(lambda ep: ep % 10 == 0))\n",
        "\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = len(SIMPLE_MOVEMENT)\n",
        "\n",
        "#print(env.observation_space.shape) #(240, 256, 3)\n",
        "\n",
        "model = Sequential()\n",
        "model.add( Permute((2,3,4,1), input_shape=(4, 240, 256, 3) ) )\n",
        "model.add( Reshape( (240, 256, 12) ) )\n",
        "model.add(Conv2D(32, (8, 8), strides=(4, 4)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (4, 4), strides=(2, 2)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3), strides=(1, 1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))\n",
        "#model.build()\n",
        "print(model.summary())\n",
        "\n",
        "#plot_model(model, to_file='./drive/ProX/model.png')\n",
        "\n",
        "memory = SequentialMemory(limit=30000, window_length=4)\n",
        "#policy = BoltzmannQPolicy()\n",
        "#policy = EpsGreedyQPolicy()\n",
        "policy = MaxBoltzmannQPolicy(eps=.8)\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=200, \n",
        "             target_model_update=1e-2, policy=policy)\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "dqn.load_weights('./drive/ProX/dqn_epsbol_{}_weights.h5f'.format(ENV_NAME))\n",
        "checkpoint_weights_filename = './drive/ProX/dqn_' + ENV_NAME + '_weights_{step}.h5f'\n",
        "log_filename = './drive/ProX/mario3/dqn_{}_log.json'.format(ENV_NAME)\n",
        "\n",
        "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000)]\n",
        "callbacks += [FileLogger(log_filename, interval=100)]\n",
        "dqn.fit(env,callbacks=callbacks, nb_steps=50000, visualize=False, verbose=2)\n",
        "dqn.save_weights('./drive/ProX/dqn_epsbol_{}_weights.h5f'.format(ENV_NAME), overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: Could not seed environment <SuperMarioBrosEnv<SuperMarioBros-v0>>\u001b[0m\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "permute_1 (Permute)          (None, 240, 256, 3, 4)    0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 240, 256, 12)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 59, 63, 32)        24608     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 59, 63, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 28, 30, 64)        32832     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 28, 30, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 26, 28, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 26, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 46592)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               23855616  \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 7)                 3591      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 7)                 0         \n",
            "=================================================================\n",
            "Total params: 23,953,575\n",
            "Trainable params: 23,953,575\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Training for 50000 steps ...\n",
            "   113/50000: episode: 1, duration: 9.091s, episode steps: 113, steps per second: 12, episode reward: 691.000, mean reward: 6.115 [-15.000, 12.000], mean action: 1.549 [0.000, 5.000], mean observation: 154.656 [0.000, 252.000], loss: --, mean_absolute_error: --, mean_q: --\n",
            "   732/50000: episode: 2, duration: 183.615s, episode steps: 619, steps per second: 3, episode reward: 2158.000, mean reward: 3.486 [-15.000, 12.000], mean action: 3.032 [0.000, 5.000], mean observation: 153.705 [0.000, 252.000], loss: 198.582023, mean_absolute_error: 320.309705, mean_q: 377.229329\n",
            "  1952/50000: episode: 3, duration: 380.354s, episode steps: 1220, steps per second: 3, episode reward: 2080.000, mean reward: 1.705 [-15.000, 12.000], mean action: 2.684 [0.000, 6.000], mean observation: 153.797 [0.000, 252.000], loss: 115.183838, mean_absolute_error: 318.364716, mean_q: 374.451294\n",
            "  2433/50000: episode: 4, duration: 147.730s, episode steps: 481, steps per second: 3, episode reward: 2289.000, mean reward: 4.759 [-15.000, 12.000], mean action: 2.661 [0.000, 6.000], mean observation: 154.476 [0.000, 252.000], loss: 66.373787, mean_absolute_error: 312.217651, mean_q: 366.504852\n",
            "  2691/50000: episode: 5, duration: 79.773s, episode steps: 258, steps per second: 3, episode reward: 1430.000, mean reward: 5.543 [-15.000, 12.000], mean action: 2.283 [0.000, 5.000], mean observation: 154.254 [0.000, 252.000], loss: 133.942825, mean_absolute_error: 313.408081, mean_q: 368.134033\n",
            "  2894/50000: episode: 6, duration: 65.599s, episode steps: 203, steps per second: 3, episode reward: 1033.000, mean reward: 5.089 [-15.000, 12.000], mean action: 2.153 [0.000, 5.000], mean observation: 154.897 [0.000, 252.000], loss: 246.520660, mean_absolute_error: 313.670593, mean_q: 368.356812\n",
            "  3426/50000: episode: 7, duration: 166.327s, episode steps: 532, steps per second: 3, episode reward: 2648.000, mean reward: 4.977 [-15.000, 12.000], mean action: 2.735 [0.000, 6.000], mean observation: 154.134 [0.000, 252.000], loss: 161.239075, mean_absolute_error: 314.851196, mean_q: 368.978973\n",
            "  3686/50000: episode: 8, duration: 81.750s, episode steps: 260, steps per second: 3, episode reward: 1211.000, mean reward: 4.658 [-15.000, 12.000], mean action: 2.596 [0.000, 5.000], mean observation: 154.111 [0.000, 252.000], loss: 125.726204, mean_absolute_error: 315.623260, mean_q: 369.910736\n",
            "  3977/50000: episode: 9, duration: 93.263s, episode steps: 291, steps per second: 3, episode reward: 1661.000, mean reward: 5.708 [-15.000, 12.000], mean action: 2.735 [0.000, 5.000], mean observation: 154.065 [0.000, 252.000], loss: 145.189667, mean_absolute_error: 315.730408, mean_q: 369.956696\n",
            "  4287/50000: episode: 10, duration: 98.670s, episode steps: 310, steps per second: 3, episode reward: 1675.000, mean reward: 5.403 [-15.000, 12.000], mean action: 2.606 [0.000, 6.000], mean observation: 153.919 [0.000, 252.000], loss: 189.428345, mean_absolute_error: 316.114227, mean_q: 370.098663\n",
            "  4528/50000: episode: 11, duration: 83.910s, episode steps: 241, steps per second: 3, episode reward: 1230.000, mean reward: 5.104 [-15.000, 12.000], mean action: 2.859 [0.000, 5.000], mean observation: 153.897 [0.000, 252.000], loss: 222.913162, mean_absolute_error: 316.356262, mean_q: 370.701843\n",
            "  4810/50000: episode: 12, duration: 95.256s, episode steps: 282, steps per second: 3, episode reward: 1846.000, mean reward: 6.546 [-15.000, 12.000], mean action: 2.535 [0.000, 6.000], mean observation: 154.749 [0.000, 252.000], loss: 191.575455, mean_absolute_error: 316.423523, mean_q: 370.667755\n",
            "  5005/50000: episode: 13, duration: 61.936s, episode steps: 195, steps per second: 3, episode reward: 1044.000, mean reward: 5.354 [-15.000, 12.000], mean action: 2.338 [0.000, 6.000], mean observation: 154.834 [0.000, 252.000], loss: 178.631851, mean_absolute_error: 318.065887, mean_q: 372.499084\n",
            "  5386/50000: episode: 14, duration: 124.539s, episode steps: 381, steps per second: 3, episode reward: 2249.000, mean reward: 5.903 [-15.000, 12.000], mean action: 2.580 [0.000, 6.000], mean observation: 154.423 [0.000, 252.000], loss: 189.511688, mean_absolute_error: 318.603210, mean_q: 373.188080\n",
            "  5905/50000: episode: 15, duration: 166.799s, episode steps: 519, steps per second: 3, episode reward: 2282.000, mean reward: 4.397 [-15.000, 12.000], mean action: 2.617 [0.000, 6.000], mean observation: 153.817 [0.000, 252.000], loss: 152.524353, mean_absolute_error: 320.742645, mean_q: 375.492004\n",
            "  6213/50000: episode: 16, duration: 97.759s, episode steps: 308, steps per second: 3, episode reward: 1818.000, mean reward: 5.903 [-15.000, 12.000], mean action: 2.419 [0.000, 6.000], mean observation: 154.746 [0.000, 252.000], loss: 212.253815, mean_absolute_error: 321.570374, mean_q: 376.240326\n",
            "  6427/50000: episode: 17, duration: 70.233s, episode steps: 214, steps per second: 3, episode reward: 1580.000, mean reward: 7.383 [-15.000, 12.000], mean action: 2.916 [0.000, 6.000], mean observation: 153.946 [0.000, 252.000], loss: 190.388138, mean_absolute_error: 322.306000, mean_q: 377.277740\n",
            "  6821/50000: episode: 18, duration: 127.199s, episode steps: 394, steps per second: 3, episode reward: 2066.000, mean reward: 5.244 [-15.000, 12.000], mean action: 2.520 [0.000, 6.000], mean observation: 154.117 [0.000, 252.000], loss: 224.857712, mean_absolute_error: 322.964752, mean_q: 377.832336\n",
            "  7446/50000: episode: 19, duration: 201.193s, episode steps: 625, steps per second: 3, episode reward: 2605.000, mean reward: 4.168 [-15.000, 12.000], mean action: 2.648 [0.000, 6.000], mean observation: 153.333 [0.000, 252.000], loss: 224.490494, mean_absolute_error: 324.327362, mean_q: 379.738037\n",
            "  7704/50000: episode: 20, duration: 83.433s, episode steps: 258, steps per second: 3, episode reward: 1506.000, mean reward: 5.837 [-15.000, 12.000], mean action: 2.876 [0.000, 6.000], mean observation: 153.972 [0.000, 252.000], loss: 143.557922, mean_absolute_error: 325.796387, mean_q: 381.531189\n",
            "  8045/50000: episode: 21, duration: 118.611s, episode steps: 341, steps per second: 3, episode reward: 1451.000, mean reward: 4.255 [-15.000, 12.000], mean action: 2.798 [0.000, 6.000], mean observation: 154.503 [0.000, 252.000], loss: 290.610779, mean_absolute_error: 325.438965, mean_q: 380.225922\n",
            "  8602/50000: episode: 22, duration: 188.396s, episode steps: 557, steps per second: 3, episode reward: 1976.000, mean reward: 3.548 [-15.000, 12.000], mean action: 2.759 [0.000, 6.000], mean observation: 153.568 [0.000, 252.000], loss: 203.577286, mean_absolute_error: 325.625946, mean_q: 380.729736\n",
            "  9354/50000: episode: 23, duration: 241.718s, episode steps: 752, steps per second: 3, episode reward: 3485.000, mean reward: 4.634 [-15.000, 12.000], mean action: 2.842 [0.000, 6.000], mean observation: 153.799 [0.000, 252.000], loss: 168.536835, mean_absolute_error: 327.262024, mean_q: 382.427887\n",
            "  9643/50000: episode: 24, duration: 92.385s, episode steps: 289, steps per second: 3, episode reward: 1225.000, mean reward: 4.239 [-15.000, 12.000], mean action: 3.225 [0.000, 6.000], mean observation: 153.941 [0.000, 252.000], loss: 233.700256, mean_absolute_error: 327.401031, mean_q: 382.845215\n",
            "  9986/50000: episode: 25, duration: 109.897s, episode steps: 343, steps per second: 3, episode reward: 2117.000, mean reward: 6.172 [-15.000, 12.000], mean action: 2.335 [0.000, 6.000], mean observation: 154.509 [0.000, 252.000], loss: 192.096710, mean_absolute_error: 328.350861, mean_q: 384.229279\n",
            " 10596/50000: episode: 26, duration: 196.494s, episode steps: 610, steps per second: 3, episode reward: 2405.000, mean reward: 3.943 [-15.000, 12.000], mean action: 2.374 [0.000, 6.000], mean observation: 153.561 [0.000, 252.000], loss: 219.720749, mean_absolute_error: 328.816040, mean_q: 384.450104\n",
            " 10968/50000: episode: 27, duration: 117.411s, episode steps: 372, steps per second: 3, episode reward: 1542.000, mean reward: 4.145 [-15.000, 12.000], mean action: 2.594 [0.000, 6.000], mean observation: 154.353 [0.000, 252.000], loss: 231.240585, mean_absolute_error: 328.311310, mean_q: 383.978149\n",
            " 11526/50000: episode: 28, duration: 178.354s, episode steps: 558, steps per second: 3, episode reward: 1861.000, mean reward: 3.335 [-15.000, 12.000], mean action: 2.591 [0.000, 6.000], mean observation: 153.526 [0.000, 252.000], loss: 224.772339, mean_absolute_error: 328.492279, mean_q: 384.062805\n",
            " 11856/50000: episode: 29, duration: 105.603s, episode steps: 330, steps per second: 3, episode reward: 1455.000, mean reward: 4.409 [-15.000, 12.000], mean action: 2.582 [0.000, 6.000], mean observation: 154.149 [0.000, 252.000], loss: 184.121017, mean_absolute_error: 328.401947, mean_q: 383.989166\n",
            " 12061/50000: episode: 30, duration: 65.965s, episode steps: 205, steps per second: 3, episode reward: 1102.000, mean reward: 5.376 [-15.000, 12.000], mean action: 2.976 [0.000, 6.000], mean observation: 154.740 [0.000, 252.000], loss: 217.706284, mean_absolute_error: 329.062561, mean_q: 384.746796\n",
            " 12593/50000: episode: 31, duration: 179.146s, episode steps: 532, steps per second: 3, episode reward: 1785.000, mean reward: 3.355 [-15.000, 12.000], mean action: 2.398 [0.000, 6.000], mean observation: 153.228 [0.000, 252.000], loss: 216.790771, mean_absolute_error: 328.565430, mean_q: 383.872925\n",
            " 12841/50000: episode: 32, duration: 84.791s, episode steps: 248, steps per second: 3, episode reward: 1429.000, mean reward: 5.762 [-15.000, 12.000], mean action: 2.484 [0.000, 6.000], mean observation: 154.919 [0.000, 252.000], loss: 151.597382, mean_absolute_error: 328.731903, mean_q: 384.171509\n",
            " 13414/50000: episode: 33, duration: 181.573s, episode steps: 573, steps per second: 3, episode reward: 2188.000, mean reward: 3.818 [-15.000, 12.000], mean action: 2.770 [0.000, 6.000], mean observation: 154.174 [0.000, 252.000], loss: 180.454239, mean_absolute_error: 328.696350, mean_q: 384.205750\n",
            " 13976/50000: episode: 34, duration: 179.845s, episode steps: 562, steps per second: 3, episode reward: 1813.000, mean reward: 3.226 [-15.000, 12.000], mean action: 2.616 [0.000, 6.000], mean observation: 153.266 [0.000, 252.000], loss: 188.432556, mean_absolute_error: 328.958679, mean_q: 384.282928\n",
            " 14370/50000: episode: 35, duration: 126.069s, episode steps: 394, steps per second: 3, episode reward: 1565.000, mean reward: 3.972 [-15.000, 12.000], mean action: 2.487 [0.000, 6.000], mean observation: 154.198 [0.000, 252.000], loss: 212.633926, mean_absolute_error: 328.012329, mean_q: 383.225250\n",
            " 14893/50000: episode: 36, duration: 170.093s, episode steps: 523, steps per second: 3, episode reward: 1773.000, mean reward: 3.390 [-15.000, 12.000], mean action: 2.618 [0.000, 6.000], mean observation: 153.531 [0.000, 252.000], loss: 176.570969, mean_absolute_error: 327.427887, mean_q: 382.517883\n",
            " 15135/50000: episode: 37, duration: 77.038s, episode steps: 242, steps per second: 3, episode reward: 1628.000, mean reward: 6.727 [-15.000, 12.000], mean action: 2.983 [0.000, 6.000], mean observation: 153.888 [0.000, 252.000], loss: 225.614639, mean_absolute_error: 326.858093, mean_q: 382.266632\n",
            " 15241/50000: episode: 38, duration: 34.107s, episode steps: 106, steps per second: 3, episode reward: 687.000, mean reward: 6.481 [-15.000, 12.000], mean action: 2.745 [0.000, 6.000], mean observation: 154.665 [0.000, 252.000], loss: 196.487778, mean_absolute_error: 326.839661, mean_q: 381.816742\n",
            " 15749/50000: episode: 39, duration: 162.648s, episode steps: 508, steps per second: 3, episode reward: 2394.000, mean reward: 4.713 [-15.000, 12.000], mean action: 2.906 [0.000, 6.000], mean observation: 153.474 [0.000, 252.000], loss: 248.539673, mean_absolute_error: 326.249176, mean_q: 381.658569\n",
            " 16373/50000: episode: 40, duration: 201.695s, episode steps: 624, steps per second: 3, episode reward: 3083.000, mean reward: 4.941 [-15.000, 12.000], mean action: 2.970 [0.000, 6.000], mean observation: 154.054 [0.000, 252.000], loss: 203.854553, mean_absolute_error: 326.264069, mean_q: 381.139435\n",
            " 16614/50000: episode: 41, duration: 87.077s, episode steps: 241, steps per second: 3, episode reward: 1547.000, mean reward: 6.419 [-15.000, 12.000], mean action: 2.975 [0.000, 6.000], mean observation: 154.515 [0.000, 252.000], loss: 153.569473, mean_absolute_error: 326.190491, mean_q: 381.257263\n",
            " 17050/50000: episode: 42, duration: 146.301s, episode steps: 436, steps per second: 3, episode reward: 1555.000, mean reward: 3.567 [-15.000, 12.000], mean action: 3.257 [0.000, 6.000], mean observation: 153.286 [0.000, 252.000], loss: 264.259308, mean_absolute_error: 325.909271, mean_q: 381.007996\n",
            " 17626/50000: episode: 43, duration: 185.288s, episode steps: 576, steps per second: 3, episode reward: 2549.000, mean reward: 4.425 [-15.000, 12.000], mean action: 2.667 [0.000, 6.000], mean observation: 153.811 [0.000, 252.000], loss: 173.193466, mean_absolute_error: 325.754272, mean_q: 381.105286\n",
            " 17830/50000: episode: 44, duration: 64.760s, episode steps: 204, steps per second: 3, episode reward: 1111.000, mean reward: 5.446 [-15.000, 12.000], mean action: 2.676 [0.000, 6.000], mean observation: 154.364 [0.000, 252.000], loss: 143.812225, mean_absolute_error: 326.652252, mean_q: 381.569611\n",
            " 18199/50000: episode: 45, duration: 115.257s, episode steps: 369, steps per second: 3, episode reward: 1192.000, mean reward: 3.230 [-15.000, 12.000], mean action: 2.748 [0.000, 6.000], mean observation: 152.789 [0.000, 252.000], loss: 211.797516, mean_absolute_error: 326.820007, mean_q: 382.053436\n",
            " 18332/50000: episode: 46, duration: 44.319s, episode steps: 133, steps per second: 3, episode reward: 1095.000, mean reward: 8.233 [-15.000, 12.000], mean action: 3.105 [0.000, 6.000], mean observation: 154.612 [0.000, 252.000], loss: 197.042175, mean_absolute_error: 327.625305, mean_q: 382.748627\n",
            " 18557/50000: episode: 47, duration: 70.223s, episode steps: 225, steps per second: 3, episode reward: 1446.000, mean reward: 6.427 [-15.000, 12.000], mean action: 2.662 [0.000, 6.000], mean observation: 154.808 [0.000, 252.000], loss: 219.425659, mean_absolute_error: 326.478546, mean_q: 381.898407\n",
            " 18881/50000: episode: 48, duration: 104.142s, episode steps: 324, steps per second: 3, episode reward: 2138.000, mean reward: 6.599 [-15.000, 12.000], mean action: 2.667 [0.000, 6.000], mean observation: 156.532 [0.000, 252.000], loss: 179.401749, mean_absolute_error: 327.124847, mean_q: 382.436493\n",
            " 19194/50000: episode: 49, duration: 98.965s, episode steps: 313, steps per second: 3, episode reward: 2111.000, mean reward: 6.744 [-15.000, 12.000], mean action: 2.681 [0.000, 6.000], mean observation: 156.797 [0.000, 252.000], loss: 209.341461, mean_absolute_error: 327.603394, mean_q: 383.232330\n",
            " 19755/50000: episode: 50, duration: 177.824s, episode steps: 561, steps per second: 3, episode reward: 2251.000, mean reward: 4.012 [-15.000, 12.000], mean action: 2.852 [0.000, 6.000], mean observation: 156.928 [0.000, 252.000], loss: 237.264954, mean_absolute_error: 327.946442, mean_q: 383.342560\n",
            " 20232/50000: episode: 51, duration: 161.456s, episode steps: 477, steps per second: 3, episode reward: 1406.000, mean reward: 2.948 [-15.000, 12.000], mean action: 2.937 [0.000, 6.000], mean observation: 155.918 [0.000, 252.000], loss: 172.796371, mean_absolute_error: 328.088318, mean_q: 383.212860\n",
            " 20472/50000: episode: 52, duration: 81.497s, episode steps: 240, steps per second: 3, episode reward: 1447.000, mean reward: 6.029 [-15.000, 12.000], mean action: 2.754 [0.000, 6.000], mean observation: 156.102 [0.000, 252.000], loss: 249.643661, mean_absolute_error: 327.972595, mean_q: 383.806274\n",
            " 20987/50000: episode: 53, duration: 163.187s, episode steps: 515, steps per second: 3, episode reward: 2064.000, mean reward: 4.008 [-15.000, 12.000], mean action: 2.709 [0.000, 6.000], mean observation: 156.197 [0.000, 252.000], loss: 266.173920, mean_absolute_error: 327.380219, mean_q: 383.041077\n",
            " 21126/50000: episode: 54, duration: 45.632s, episode steps: 139, steps per second: 3, episode reward: 1057.000, mean reward: 7.604 [-15.000, 12.000], mean action: 3.058 [0.000, 5.000], mean observation: 155.171 [0.000, 252.000], loss: 207.213715, mean_absolute_error: 327.057281, mean_q: 382.429474\n",
            " 21345/50000: episode: 55, duration: 69.461s, episode steps: 219, steps per second: 3, episode reward: 1445.000, mean reward: 6.598 [-15.000, 12.000], mean action: 2.635 [0.000, 6.000], mean observation: 155.734 [0.000, 252.000], loss: 158.673798, mean_absolute_error: 327.318695, mean_q: 382.347992\n",
            " 21625/50000: episode: 56, duration: 89.578s, episode steps: 280, steps per second: 3, episode reward: 1173.000, mean reward: 4.189 [-15.000, 12.000], mean action: 2.629 [0.000, 6.000], mean observation: 155.182 [0.000, 252.000], loss: 258.308990, mean_absolute_error: 327.097595, mean_q: 382.230469\n",
            " 21923/50000: episode: 57, duration: 95.543s, episode steps: 298, steps per second: 3, episode reward: 1545.000, mean reward: 5.185 [-15.000, 12.000], mean action: 2.544 [0.000, 6.000], mean observation: 156.917 [0.000, 252.000], loss: 202.986343, mean_absolute_error: 327.825653, mean_q: 383.434326\n",
            " 22247/50000: episode: 58, duration: 104.845s, episode steps: 324, steps per second: 3, episode reward: 1637.000, mean reward: 5.052 [-15.000, 12.000], mean action: 2.722 [0.000, 6.000], mean observation: 156.040 [0.000, 252.000], loss: 203.182312, mean_absolute_error: 328.281860, mean_q: 383.986633\n",
            " 22493/50000: episode: 59, duration: 79.390s, episode steps: 246, steps per second: 3, episode reward: 1416.000, mean reward: 5.756 [-15.000, 12.000], mean action: 2.728 [0.000, 6.000], mean observation: 156.437 [0.000, 252.000], loss: 274.890625, mean_absolute_error: 328.454773, mean_q: 383.748169\n",
            " 22882/50000: episode: 60, duration: 123.878s, episode steps: 389, steps per second: 3, episode reward: 1478.000, mean reward: 3.799 [-15.000, 12.000], mean action: 2.784 [0.000, 6.000], mean observation: 157.178 [0.000, 252.000], loss: 215.924118, mean_absolute_error: 328.216125, mean_q: 384.095612\n",
            " 23186/50000: episode: 61, duration: 105.297s, episode steps: 304, steps per second: 3, episode reward: 1866.000, mean reward: 6.138 [-15.000, 12.000], mean action: 2.382 [0.000, 6.000], mean observation: 156.241 [0.000, 252.000], loss: 241.452103, mean_absolute_error: 328.659180, mean_q: 384.325470\n",
            " 23705/50000: episode: 62, duration: 175.037s, episode steps: 519, steps per second: 3, episode reward: 2065.000, mean reward: 3.979 [-15.000, 12.000], mean action: 2.927 [0.000, 6.000], mean observation: 155.632 [0.000, 252.000], loss: 151.963226, mean_absolute_error: 329.967651, mean_q: 386.043884\n",
            " 24047/50000: episode: 63, duration: 111.454s, episode steps: 342, steps per second: 3, episode reward: 2239.000, mean reward: 6.547 [-15.000, 12.000], mean action: 2.968 [0.000, 6.000], mean observation: 156.488 [0.000, 252.000], loss: 174.477112, mean_absolute_error: 331.411835, mean_q: 387.710632\n",
            " 24437/50000: episode: 64, duration: 124.312s, episode steps: 390, steps per second: 3, episode reward: 1836.000, mean reward: 4.708 [-15.000, 12.000], mean action: 3.128 [0.000, 6.000], mean observation: 156.843 [0.000, 252.000], loss: 168.718750, mean_absolute_error: 333.361359, mean_q: 389.945038\n",
            " 24870/50000: episode: 65, duration: 138.850s, episode steps: 433, steps per second: 3, episode reward: 1846.000, mean reward: 4.263 [-15.000, 12.000], mean action: 2.716 [0.000, 6.000], mean observation: 156.445 [0.000, 252.000], loss: 266.880524, mean_absolute_error: 334.427155, mean_q: 391.013000\n",
            " 25425/50000: episode: 66, duration: 176.949s, episode steps: 555, steps per second: 3, episode reward: 2092.000, mean reward: 3.769 [-15.000, 12.000], mean action: 2.521 [0.000, 6.000], mean observation: 157.053 [0.000, 252.000], loss: 248.656982, mean_absolute_error: 334.750000, mean_q: 391.586426\n",
            " 25852/50000: episode: 67, duration: 137.280s, episode steps: 427, steps per second: 3, episode reward: 1924.000, mean reward: 4.506 [-15.000, 12.000], mean action: 2.478 [0.000, 6.000], mean observation: 156.620 [0.000, 252.000], loss: 223.400330, mean_absolute_error: 334.819305, mean_q: 391.220581\n",
            "done, took 8470.194 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q2WbAle2X57P",
        "colab_type": "code",
        "outputId": "505a96aa-fc15-4806-a9f7-7b984c11a204",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 847
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import keras\n",
        "#import pydot\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Conv2D, Reshape, MaxPooling2D, Permute\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import plot_model\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy, MaxBoltzmannQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
        "\n",
        "from nes_py.wrappers import BinarySpaceToDiscreteSpaceEnv\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
        "\n",
        "from gym import wrappers\n",
        "\n",
        "from keras import backend as K # 柴田\n",
        "\n",
        "\n",
        "ENV_NAME = 'SuperMarioBros-v0'\n",
        "\n",
        "# Get the environment and extract the number of actions.\n",
        "env = gym_super_mario_bros.make(ENV_NAME)\n",
        "env = BinarySpaceToDiscreteSpaceEnv(env, SIMPLE_MOVEMENT)\n",
        "\n",
        "env = wrappers.Monitor(env, './drive/ProX/mario3', force=False, video_callable=(lambda ep: ep % 1 == 0))\n",
        "\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = len(SIMPLE_MOVEMENT)\n",
        "\n",
        "#print(env.observation_space.shape) #(240, 256, 3)\n",
        "\n",
        "model = Sequential()\n",
        "model.add( Permute((2,3,4,1), input_shape=(4, 240, 256, 3) ) )\n",
        "model.add( Reshape( (240, 256, 12) ) )\n",
        "model.add(Conv2D(32, (8, 8), strides=(4, 4)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (4, 4), strides=(2, 2)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3), strides=(1, 1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))\n",
        "#model.build()\n",
        "print(model.summary())\n",
        "\n",
        "#plot_model(model, to_file='./drive/ProX/model.png')\n",
        "\n",
        "memory = SequentialMemory(limit=30000, window_length=4)\n",
        "#policy = BoltzmannQPolicy()\n",
        "#policy = EpsGreedyQPolicy()\n",
        "policy = MaxBoltzmannQPolicy(eps=.8)\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=200, \n",
        "             target_model_update=1e-2, policy=policy)\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "dqn.load_weights('./drive/ProX/dqn_epsbol_{}_weights.h5f'.format(ENV_NAME))\n",
        "checkpoint_weights_filename = './drive/ProX/dqn_' + ENV_NAME + '_weights_{step}.h5f'\n",
        "log_filename = './drive/ProX/mario3/dqn_{}_log.json'.format(ENV_NAME)\n",
        "\n",
        "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000)]\n",
        "callbacks += [FileLogger(log_filename, interval=100)]\n",
        "dqn.test(env, nb_episodes=5, visualize=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: Could not seed environment <SuperMarioBrosEnv<SuperMarioBros-v0>>\u001b[0m\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "permute_1 (Permute)          (None, 240, 256, 3, 4)    0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 240, 256, 12)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 59, 63, 32)        24608     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 59, 63, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 28, 30, 64)        32832     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 28, 30, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 26, 28, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 26, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 46592)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               23855616  \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 7)                 3591      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 7)                 0         \n",
            "=================================================================\n",
            "Total params: 23,953,575\n",
            "Trainable params: 23,953,575\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: 666.000, steps: 121\n",
            "Episode 2: reward: 666.000, steps: 121\n",
            "Episode 3: reward: 666.000, steps: 121\n",
            "Episode 4: reward: 666.000, steps: 121\n",
            "Episode 5: reward: 666.000, steps: 121\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4e88f89940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "jD2XNlwYBn_s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get -qq install -y graphviz && pip install -q pydot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lJMLIo82tKJ6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 前のやつ"
      ]
    },
    {
      "metadata": {
        "id": "P6g7k1w3z1iL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import keras\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Conv2D\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import BoltzmannQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
        "\n",
        "from nes_py.wrappers import BinarySpaceToDiscreteSpaceEnv\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
        "\n",
        "from gym import wrappers # <=追加\n",
        "\n",
        "\n",
        "ENV_NAME = 'SuperMarioBros-v0'\n",
        "\n",
        "# Get the environment and extract the number of actions.\n",
        "env = gym_super_mario_bros.make(ENV_NAME)\n",
        "env = BinarySpaceToDiscreteSpaceEnv(env, SIMPLE_MOVEMENT)\n",
        "\n",
        "env = wrappers.Monitor(env, './drive/mario', force=True, video_callable=(lambda ep: ep % 10 == 0))  # <=追加\n",
        "\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = len(SIMPLE_MOVEMENT)\n",
        "\n",
        "# Next, we build a very simple model.\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))\n",
        "print(model.summary())\n",
        "\n",
        "memory = SequentialMemory(limit=50000, window_length=1)\n",
        "policy = BoltzmannQPolicy()\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=100,\n",
        "             target_model_update=1e-2, policy=policy)\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "dqn.load_weights('./drive/dqn_{}_weights.h5f'.format(ENV_NAME))\n",
        "checkpoint_weights_filename = 'dqn_' + ENV_NAME + '_weights_{step}.h5f'\n",
        "log_filename = 'dqn_{}_log.json'.format(ENV_NAME)\n",
        "\n",
        "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000)]\n",
        "callbacks += [FileLogger(log_filename, interval=100)]\n",
        "dqn.fit(env,callbacks=callbacks, nb_steps=400000, visualize=False, verbose=2)\n",
        "dqn.save_weights('./drive/dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BOkrflmiWGJn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp dqn_SuperMarioBros-v0_log.json ./drive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2N28G660OFFn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"dqn_SuperMarioBros-v0_log.json\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U9fkwnHM9Gaw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get -qq install -y graphviz && pip install -q pydot\n",
        "import pydot\n",
        "from keras.utils import plot_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5j96YsRfTpFA",
        "colab_type": "code",
        "outputId": "51638735-31cb-43ee-ec6f-624badfb87ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.utils.visualize_util import plot\n",
        "plot(dqn, to_file=\"model.png\", show_shapes=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-439e3c6cabfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdqn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"model.png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.utils.visualize_util'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "9FQuFk2jPHQZ",
        "colab_type": "code",
        "outputId": "d43f549f-191a-4c50-940e-ecf111ccb49a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "cell_type": "code",
      "source": [
        "env = wrappers.Monitor(env, './drive/mario2', force=True, video_callable=(lambda ep: ep % 1 == 0))  # <=追加\n",
        "dqn.load_weights('dqn_{}_weights.h5f'.format(ENV_NAME))\n",
        "dqn.test(env, nb_episodes=5, visualize=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0239a6c60ad3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./drive/mario2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_callable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mep\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# <=追加\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dqn_{}_weights.h5f'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mENV_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'wrappers' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "uBtc4vhzfhhZ",
        "colab_type": "code",
        "outputId": "4a439a5f-4212-4293-c8bd-c77fc3d2d03c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import BoltzmannQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "\n",
        "from nes_py.wrappers import BinarySpaceToDiscreteSpaceEnv\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
        "\n",
        "from gym import wrappers # <=追加\n",
        "\n",
        "ENV_NAME = 'SuperMarioBros-v0'\n",
        "\n",
        "# Get the environment and extract the number of actions.\n",
        "env = gym_super_mario_bros.make(ENV_NAME)\n",
        "env = BinarySpaceToDiscreteSpaceEnv(env, SIMPLE_MOVEMENT)\n",
        "\n",
        "env = wrappers.Monitor(env, './drive/mario2', force=True, video_callable=(lambda ep: ep % 1 == 0))  # <=追加\n",
        "\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = len(SIMPLE_MOVEMENT)\n",
        "\n",
        "# Next, we build a very simple model.\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))\n",
        "print(model.summary())\n",
        "\n",
        "memory = SequentialMemory(limit=50000, window_length=1)\n",
        "policy = BoltzmannQPolicy()\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=0,\n",
        "               target_model_update=1e-2, policy=policy)\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "dqn.load_weights('./drive/dqn_{}_weights.h5f'.format(ENV_NAME))\n",
        "dqn.test(env, nb_episodes=5, visualize=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: Could not seed environment <SuperMarioBrosEnv<SuperMarioBros-v0>>\u001b[0m\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2 (Flatten)          (None, 184320)            0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 16)                2949136   \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 7)                 119       \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 7)                 0         \n",
            "=================================================================\n",
            "Total params: 2,949,799\n",
            "Trainable params: 2,949,799\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Testing for 5 episodes ...\n",
            "Episode 1: reward: 732.000, steps: 82\n",
            "Episode 2: reward: 732.000, steps: 82\n",
            "Episode 3: reward: 732.000, steps: 82\n",
            "Episode 4: reward: 732.000, steps: 82\n",
            "Episode 5: reward: 732.000, steps: 82\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f680a1a1710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "03w7beFQMESd",
        "colab_type": "code",
        "outputId": "f0a13b15-1267-4e96-920c-f9ad6aa25d88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        }
      },
      "cell_type": "code",
      "source": [
        "env = wrappers.Monitor(env, './drive/mario2', force=True, video_callable=(lambda ep: ep % 1 == 0))  # <=追加\n",
        "\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
        "               target_model_update=1e-2, policy=policy)\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "dqn.load_weights('dqn_{}_weights.h5f'.format(ENV_NAME))\n",
        "\n",
        "# Finally, evaluate our algorithm for 5 episodes.\n",
        "dqn.test(env, nb_episodes=5, visualize=False)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 5 episodes ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-521ddcbb7b30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Finally, evaluate our algorithm for 5 episodes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/rl/core.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0;31m# Obtain the initial observation by resetting the environment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m_before_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitoring/stats_recorder.py\u001b[0m in \u001b[0;36mbefore_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tried to reset environment which is not done. While the monitor is active for {}, you cannot call reset() unless the episode is over.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'NoneType' and 'int'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "oim4p7kb7pcu",
        "colab_type": "code",
        "outputId": "b0a755c4-a685-4ee1-ea00-6b263452a89f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 913
        }
      },
      "cell_type": "code",
      "source": [
        "import rl.callbacks\n",
        "class EpisodeLogger(rl.callbacks.Callback):\n",
        "    def __init__(self):\n",
        "        self.observations = {}\n",
        "        self.rewards = {}\n",
        "        self.actions = {}\n",
        "\n",
        "    def on_episode_begin(self, episode, logs):\n",
        "        self.observations[episode] = []\n",
        "        self.rewards[episode] = []\n",
        "        self.actions[episode] = []\n",
        "\n",
        "    def on_step_end(self, step, logs):\n",
        "        episode = logs['episode']\n",
        "        self.observations[episode].append(logs['observation'])\n",
        "        self.rewards[episode].append(logs['reward'])\n",
        "        self.actions[episode].append(logs['action'])\n",
        "\n",
        "env = wrappers.Monitor(env, './drive/mario2', force=True, video_callable=(lambda ep: ep % 1 == 0))\n",
        "cb_ep = EpisodeLogger()\n",
        "dqn.test(env, nb_episodes=10, visualize=False, callbacks=[cb_ep])\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for obs in cb_ep.observations.values():\n",
        "    plt.plot([o[0] for o in obs])\n",
        "plt.xlabel(\"step\")\n",
        "plt.ylabel(\"pos\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for 10 episodes ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-be55502c3d39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./drive/mario2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_callable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mep\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mcb_ep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEpisodeLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcb_ep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/rl/core.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0;31m# Obtain the initial observation by resetting the environment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m_before_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitoring/stats_recorder.py\u001b[0m in \u001b[0;36mbefore_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tried to reset environment which is not done. While the monitor is active for {}, you cannot call reset() unless the episode is over.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: Tried to reset environment which is not done. While the monitor is active for SuperMarioBros-v0, you cannot call reset() unless the episode is over."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "bMt5Teqw27bT",
        "colab_type": "code",
        "outputId": "9927669a-a823-49c5-b8a0-ae525719e1a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1013'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1013'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    }
  ]
}